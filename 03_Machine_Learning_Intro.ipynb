{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b9880fe",
   "metadata": {},
   "source": [
    "# Machine Learning Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73722808",
   "metadata": {},
   "source": [
    "## TEIL A: Klassifikation von Iris Blumen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7b22c9",
   "metadata": {},
   "source": [
    "In dieser Aufgabe sehen wir uns das Iris Dataset an. Hierbei geht es darum die Blumen anhand ihrer Blütenblätter- (Petal) und Kelchblättermasse (Sepal) zu klassifizieren.\n",
    "\n",
    "Wir importieren zuerst einmal einige Libraries die wir nutzen möchten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef4cceb",
   "metadata": {},
   "source": [
    "<div style=\"padding: 5px; border: 5px solid #a10000ff;\">\n",
    "\n",
    "**Hinweis:** In den Codezellen sind jeweils einige Codeteile nicht programmiert. Diesen Code müssen Sie ergänzen. Die jeweiligen Stellen sind mit einem Kommentar und dem Keyword **TODO** vermerkt und z.T. Stellen mit ... markiert.\n",
    "\n",
    "Ausserdem gibt es einige assert Statements. Diese geben einen Fehler aus, sollte etwas bei Ihrer Programmierung nicht korrekt sein."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab85211",
   "metadata": {},
   "source": [
    "**Installation:** Führen Sie folgende Zeile aus um die hier benötigten Libraries zu installieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb3098f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas matplotlib seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667c676c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734d087d",
   "metadata": {},
   "source": [
    "Nun laden wir das Iris Dataset, welches bereits in der Library angeboten wird. Wir speichern es auch in einem DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a7b9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load iris dataset\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "#create pandas dataframe from iris\n",
    "df_iris = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df_iris['target'] = iris.target\n",
    "df_iris['target_names'] = df_iris['target'].apply(lambda x: iris.target_names[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd009e4",
   "metadata": {},
   "source": [
    "Eine Aufgabe, die von Machine Learning übernommen wird, ist die Klassifizierung. \n",
    "\n",
    "Dabei ist es die Aufgabe ein **Data Sample** (z.B. eine Katze oder einen Hund) aufgrund von bestimmten **Features** (z.B. Anzahl Streifen, Grösse) einer bestimmten Kategorie auch **Klasse** genannt zuzuweisen. Die einzelnen Data Samples sind jeweils mit einem **Label** gekennzeichnet, zu welcher Klasse sie gehören."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761d8acf",
   "metadata": {},
   "source": [
    "### Aufgabe 1\n",
    "Zeigen Sie nun das Pandas Dataframe mit den Daten an.\n",
    "\n",
    "**Fragen**\n",
    "\n",
    "Was sind die Features die wir in diesem Beispiel nutzen?\n",
    "\n",
    "\n",
    "Was beinhaltet die Spalte target bzw. target_names?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8406671",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Zeigen Sie hier das Pandas DataFrame an\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16ef30a",
   "metadata": {},
   "source": [
    "### Optional: Aufgabe 2\n",
    "Wir zeigen nun die paarweise Kombination von den Features an. Führen Sie dazu die nächste Code-Zelle aus.\n",
    "\n",
    "**Frage**\n",
    "\n",
    "Wenn Sie die Blumen von Auge anhand eines der Diagramme unterscheiden müssten.\n",
    "Welche Feature Kombination würde sich gut eignen? Begründen Sie.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1728b5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wir plotten den Featurespace von jeweils zwei Features als Dimensionen\n",
    "pairplt = sns.pairplot(df_iris.drop('target', axis=1), hue='target_names', height=2, palette='tab10')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff3350c",
   "metadata": {},
   "source": [
    "### Aufgabe 3\n",
    "In diesem Fall möchten wir nun die Blumen in die Klassen setosa (0), versicolor (1) und verginica (2) einteilen. Die Daten haben alle bereits die sogenannten **Labels** zugewiesen. Diese werden manchmal auch target genannt und sind in den Daten in den Spalten target und target_names vorhanden.\n",
    "\n",
    "Wenn wir ein oder mehrere Features in einem mathematischen Raum kombinieren, entsteht ein sogennanter **Feature Space** (Merkmalsraum).\n",
    "Ein solcher Feature Space für die *petal width* und *petal length* sieht wie folgt aus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4129342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Vervollständigen Sie die Spaltennamen der Parameter x und y für einen Scatterplot der Petal Length gegen die Petal Width\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(..., ..., c=df_iris['target'], cmap='viridis')\n",
    "ax.set_xlabel('Petal Length (cm)')\n",
    "ax.set_ylabel('Petal Width (cm)')\n",
    "ax.set_title('Iris Petal Length vs Width') \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60405aaa",
   "metadata": {},
   "source": [
    "\n",
    "**Fragen**\n",
    "\n",
    "Wie viele Dimensionen hat der Feature Space im obigen Beispiel?\n",
    "\n",
    "\n",
    "Wie viele Dimensionen können wir in einem Feature Space haben?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83762628",
   "metadata": {},
   "source": [
    "### Aufgabe 4\n",
    "\n",
    "Ein Klassifikationsmodell versucht eine oder mehrere **Decision Boundaries** also Entscheidungsgrenzen in den Feature Space zu platzieren, so dass möglichst viele Data Samples korrekt klassifiziert werden.\n",
    "Die Entscheidungsgrenze unterteilt den Feature Space (Merkmalsraum) in zwei Teilräume. Auf der einen Seite der Grenze sind dann Data Samples der einen Klasse und auf der anderen Seite die der anderen Klasse.\n",
    "\n",
    "**Frage**\n",
    "\n",
    "Beschreiben Sie, wie Sie zwei linearen Decision Boundaries (gerade Linien) im Feature Space oben platzieren würden um die drei Klassen möglichst gut auseinanderzuhalten.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6905907d",
   "metadata": {},
   "source": [
    "### Aufgabe 5\n",
    "\n",
    "Machine Learning Modelle treffen Entscheidungen anhand von Daten, welche Sie zu Beginn der «Kalibrierung» genutzt haben, um zu lernen. Man nennt diese Daten **Trainingsdaten**, da das Modell sozusagen damit trainiert wird. Wenn nun ein Machine Learning Modell getestet wird (heisst: wie gut funktioniert das Modell?), werden ihm neue Daten sogenannte **Testdaten** präsentiert, welche nicht für das Training genutzt wurden. \n",
    "\n",
    "**Fragen**\n",
    "\n",
    "Was ist der Vorteil der Nutzung von Testdaten? \n",
    "\n",
    "\n",
    "Was könnte ein Nachteil oder eine Herausforderung sein?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe25c47",
   "metadata": {},
   "source": [
    "### Aufgabe 6 \n",
    "\n",
    "Damit wir Trainings- und Testdaten haben, teilen wir die Daten in Trainings und Testdaten ein. Das gleiche passiert auch mit den targets. Wir nutzen eine fertige Funktion von sklearn dafür.\n",
    "Der Parameter test_size gibt an, welcher Bruchteil der gesamten Daten als Testdaten genutzt werden soll.\n",
    "\n",
    "Vielfach werden die Features in einer Variable X gespeichert und die Labels bzw. Targets in einer Variable y.\n",
    "\n",
    "Wir machen das gleich und kennzeichnen gleich im Variablennamen auch ob die Daten die Trainings oder Testdaten sind.\n",
    "\n",
    "**Fragen**\n",
    "\n",
    "\n",
    "Wie viele Data Samples sind nun im Trainings Dataset und wie viele im Testdataset?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de847c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_iris_train, X_iris_test, y_iris_train, y_iris_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
    "\n",
    "#TODO Ausgabe der Anzahl der Trainings- und Testdaten\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20a0241",
   "metadata": {},
   "source": [
    "### Aufgabe 7\n",
    "\n",
    "Wir trainieren unseren ersten Classifier und testen diesen auch. \n",
    "\n",
    "Vorerst nutzen wir einen fixfertigen Classifier von Sklearn den wir instanziieren müssen und dann \"fitten\". Hinter der fit Funktion versteckt sich ein Trainingsprozess in dem das Modell von den Trainingsdaten lernt.\n",
    "Vervollständige die Befehle anhand der Kommentare.\n",
    "\n",
    "**Bemerkung:** Das Modell wurde extra so konfiguriert, dass nicht eine 100% Genauigkeit resultiert, damit wir die Genauigkeit auswerten können. Deshalb erscheint auch eine **ConvergenceWarning**, diese können Sie **ignorieren**. Die Warnung bedeutet, dass das Modell noch nicht die optimalen Parameter gelernt hat und noch weiter lernen sollte.\n",
    "\n",
    "Wir evaluieren das Modell mittels der Accuracy. Diese wird berechnet indem die Anzahl korrekt klassifizierten Data Samples geteilt durch die Anzahl aller Data Samples gerechnet wird:\n",
    "\n",
    "  $ Accuracy = \\frac{\\text{Anzahl korrekt Klassifizierte Data Samples}}{\\text{Alle Data Samples}} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9194b47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(10,), max_iter=500, random_state=42)\n",
    "\n",
    "#TODO Trainieren Sie das Modell mit den Trainingsdaten als erstes Argument und den zugehörigen Labels als zweites Argument\n",
    "mlp.fit(..., ...)\n",
    "\n",
    "#TODO Berechnen Sie mit dem Modell die Klassen für die Testdaten als einzigen Parameter in der predict-Methode\n",
    "y_iris_pred = mlp.predict(...)\n",
    "\n",
    "# TODO Berechnen Sie die Genauigkeit des Modells\n",
    "# Dies können wir berechnen indem wir zählen, wie viele der vorhergesagten Labels mit den tatsächlichen Labels übereinstimmen und dies durch die Gesamtanzahl der Testdaten teilen\n",
    "accuracy = ...\n",
    "\n",
    "# Test ob die Accuracy korrekt berechnet wurde\n",
    "assert (abs(accuracy-0.93) < 0.01), \"Die Genauigkeit des Modells ist nicht korrekt berechnet.\"\n",
    "print(f'Accuracy: {accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9171e4",
   "metadata": {},
   "source": [
    "### Optional: Aufgabe 8\n",
    "\n",
    "Wir schauen uns nun noch genauer an, welche Klassen am meisten verwechselt wurden. Dazu werden wir eine Konfusionsmatrix nutzen. Dabei werden die tatsächlichen Labels den vorhergesagten Labels gegenübergestellt und aufgezeichnet, wie oft jede Kombination vorkommt.\n",
    "\n",
    "\n",
    "**Frage**\n",
    "Betrachten Sie nun nochmals die paarweisen Feature Spaces aus Aufgabe 2 betrachten.\n",
    "Welche zwei Klassen werden wohl am meisten verwechselt?\n",
    "\n",
    "\n",
    "Lassen Sie nun die Zelle unten laufen und interpretieren Sie die Konfusionsmatrix. Stimmte ihre Vermutung?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc1a616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wir zeigen nun eine Konfusionsmatrix an, um die Leistung des Modells zu visualisieren. \n",
    "\n",
    "conf_matrix = ConfusionMatrixDisplay.from_predictions(y_iris_test, y_iris_pred, display_labels=iris.target_names)\n",
    "conf_matrix.ax_.set_title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ac9a04",
   "metadata": {},
   "source": [
    "## Kontrollfragen Klassifikation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4147ef13",
   "metadata": {},
   "source": [
    "**Kontrollfrage 1**\n",
    "\n",
    "Ein Machine-Learning Modell klassifiziert die Schüler*innen ob sie mit dem Velo, den ÖV oder zu Fuss zur Schule kommen. \n",
    "In den Daten wird pro Schüler*in und Tag erfasst, wie weit der Schulweg ist, wie lange die Reisedauer war, ob der/die Schüler*in ein Velo besitzt und wie weit die nächste ÖV-Haltestelle vom Wohnort entfernt ist.\n",
    "\n",
    "Was sind in diesem Beispiel\n",
    "\n",
    "    - Features?\n",
    "    - Klassen?\n",
    "    - Data Samples?\n",
    "    \n",
    "Was wäre ein Beispiel für eine Ein und Ausgabe des Modells?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Kontrollfrage 2**\n",
    "\n",
    "Beschreiben Sie das Vorgehen, um ein Klassifikationsmodell zu evaluieren und dabei die Accuracy zu berechnen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64193641",
   "metadata": {},
   "source": [
    "## TEIL B: Optional Regression mit Hauspreisberechnung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e0ed32",
   "metadata": {},
   "source": [
    "In diesem Dataset wurden verschiedene Merkmale (Features) von Liegenschaften erfasst, wobei die Angaben meist pro Block erfasst sind.\n",
    "\n",
    "Dabei soll nun von den Merkmalen auf den Hauspreis geschlossen werden. Der Hauspreis ist somit die **Zielvariable** oder engl. *Target*, ähnlich dem Label in der Klassifikation.\n",
    "\n",
    "Die Berechnungen des Hauspreises, werden wir mit einem Regressionsmodell machen.\n",
    "\n",
    "Das Dataset das wir benutzten, ist das California Housing Dataset:\n",
    "https://media.geeksforgeeks.org/wp-content/uploads/20240522145850/housing%5B1%5D.csv\n",
    "\n",
    "\n",
    "**Das Dataset hat folgende Features:**\n",
    "1. longitude: Koordinaten wie westlich das Haus liegt, höhrere Werte sind weiter westlich.\n",
    "2. latitude: Koordinaten wie nördlich das Haus liegt, höhrere Werte sind weiter nördlich.\n",
    "3. housingMedianAge: Median Alter des Hauses in einem Block, tiefere Werte heisst neueres Haus.\n",
    "4. totalRooms: Anzahl Räume innerhalb eines Blocks.\n",
    "5. totalBedrooms: Anzahl Schlafzimmer innerhalb eines Blocks.\n",
    "6. population: Anzahl Personen pro Block.\n",
    "7. households: Gesamtzahl der Haushalte, d. h. der Gruppen von Personen, die innerhalb einer Wohneinheit wohnen, für einen Block\n",
    "8. medianIncome: Medianeinkommen der Haushalte innerhalb eines Häuserblocks (gemessen in Zehntausend US-Dollar)\n",
    "9. medianHouseValue: Medianwert der Häuser für Haushalte innerhalb eines Blocks (gemessen in US-Dollar)\n",
    "10. oceanProximity: Lage des Hauses in Bezug auf den Ozean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712f7376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wir laden das Housing Dataset in ein Pandas DataFrame\n",
    "df_housing = pd.read_csv(\"datasets/housing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b0b1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_housing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e65082",
   "metadata": {},
   "source": [
    "### Aufgabe 9\n",
    "\n",
    "Sie haben sich sicherlich die Features im Dataframe angeschaut. Machine Learning Modelle benötigen die Daten als Zahlen um diese im Features Space abbilden zu können. Jedoch haben wir mit ocean_proximity ein Feature das Kategorische Daten enthält. Diese können wir mit dem sogenannten One-Hot-Encoding in einen mathematischen Raum übertragen. Dies geschieht indem wir für jede Kategorie eine neue Dimension anlegen und dort eine 1 vermerken wenn die Kategorie zutrifft und bei allen anderen eine 0. Wir nutzen dazu den One-Hot-Encoder von Scikit-learn.\n",
    "\n",
    "Zusätzlich entfernen wir noch alle Data Samples die leere Werte haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030101bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wir verwenden OneHotEncoder aus sklearn.preprocessing\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# TODO Konfigurieren Sie das One-Hot-Encoding auf der Spalte 'ocean_proximity' indem Sie das DataFrame mit der Spaltenangabe als Parameter einfügst. Beispiel: ohe.fit(df_iris[['petal length (cm)']])\n",
    "ohe.fit(...)\n",
    "\n",
    "# Wir erstellen ein neues DataFrame mit den kodierten Spalten und füge sie dem ursprünglichen DataFrame hinzu. Danach entfernen wir die ursprüngliche Spalte 'ocean_proximity'.\n",
    "df_housing_encoded = pd.concat([df_housing, pd.DataFrame(ohe.transform(df_housing[['ocean_proximity']]), columns=ohe.get_feature_names_out(['ocean_proximity']))], axis=1)\n",
    "df_housing_encoded.drop('ocean_proximity', axis=1, inplace=True)\n",
    "\n",
    "# Wir entfernen Zeilen mit fehlenden Werten, da diese nicht für das Training des Modells verwendet werden können\n",
    "df_housing_encoded.dropna(inplace=True)\n",
    "\n",
    "df_housing_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49801d8",
   "metadata": {},
   "source": [
    "### Optional: Aufgabe 10\n",
    "\n",
    "Wir möchten nun noch die Daten normalisieren. Dies bedeutet, dass alle Features danach den gleichen Wertebereich haben. \n",
    "Dies hilft einigen Modellen zum Beispiel künstlichen Neuronalen Netzwerken schneller zu optimieren und zu lernen.\n",
    "\n",
    "Wir wenden die min-max-Skalierung an um die Daten zu normalisieren.\n",
    "Das heisst alle Features haben danach einen minimalen Wert von 0 und einen maximalen Wert von 1.\n",
    "\n",
    "Wie könnten Sie dies berrechnen? Vervollständige danach den Code unten.\n",
    "\n",
    "<details>\n",
    "<summary><b>Tipp 1:</b> Klicke hier für den ersten Tipp.</summary>\n",
    "\n",
    "Wie nutzen Sie das Minimum eines Features und das Maximum damit nachher alle Werte eines Features zwischen (inklusive) 0 und 1 sind?\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary><b>Lösung:</b> Klicken Sie hier um die Formel anzuzeigen.</summary>\n",
    "\n",
    "$scaled\\_value = \\frac{value-min}{max - min}$\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9c7f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisieren der numerischen Features mit Min-Max-Skalierung\n",
    "\n",
    "# Wir haben eine Liste von numerischen Features\n",
    "numerical_features = df_housing_encoded.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# TODO Normalisieren Sie die numerischen Features mit Min-Max-Skalierung\n",
    "for feature in numerical_features:\n",
    "    ...\n",
    "\n",
    "df_housing_encoded\n",
    "\n",
    "# Prüfen ob die numerischen Features korrekt normalisiert wurden\n",
    "assert (df_housing_encoded[numerical_features].min().min() >= 0) and (df_housing_encoded[numerical_features].max().max() <= 1), \"Die numerischen Features wurden nicht korrekt normalisiert.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a48e1e",
   "metadata": {},
   "source": [
    "### Aufgabe 11\n",
    "\n",
    "Unterteilen Sie das Dataset in ein Trainings und Testteil wie im vorherigen Abschnitt Teil A bereits gemacht.\n",
    "Nutzen Sie auch einen Train/Test Split von 80/20 und den Random State 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc6c3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Unterteilen sie das Dataset in Trainigns- und Testdaten. Die Spalte 'median_house_value' ist die Zielvariable, die wir vorhersagen möchten. Deshalb wird sie von den Features getrennt. Entferne die Zielvariable aus den Features bei der Parameterübergabe mit df_housing_encoded.drop('median_house_value', axis=1) und benutze sie als zweiten Parameter in der train_test_split Funktion.\n",
    "\n",
    "X_housing_train, X_housing_test, y_housing_train, y_housing_test = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe772d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Lassen Sie diese Tests laufen, um zu prüfen ob die Aufteilung korrekt ist\n",
    "assert X_housing_train.shape[0] == 16346, f\"Erwartete Anzahl Trainingsdaten: 16346, aktuell sind es: {X_housing_train.shape[0]}\"\n",
    "assert X_housing_test.shape[0] == 4087 , f\"Erwartete Anzahl Testdaten: 4087, aktuell sind es: {X_housing_test.shape[0]}\"\n",
    "\n",
    "# Prüfen Sie ob median_house_value aus den Features entfernt wurde\n",
    "assert 'median_house_value' not in X_housing_train.columns, \"median_house_value wurde nicht aus den Features entfernt.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0690ff11",
   "metadata": {},
   "source": [
    "### Aufgabe 12\n",
    "\n",
    "1. Nutzen Sie die MLPRegressor Klasse um ein Modell zu instantieren. Die Klasse wurde bereits am Anfang importiert. Sie können die gleichen Parameter verwenden wie in Aufgabe 7 beim MLPClassifier.\n",
    "2. Trainieren Sie nun das Modell mit dem Aufruf der fit(Trainingsdaten, Targets) Methode.\n",
    "\n",
    "Optional: Weitere Infos zur MLPRegressor Klasse: https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45141773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Erstellen des MLPRegressor Modells mit den gleichen Parametern wie Aufgabe 7.\n",
    "mlp_regressor = MLPRegressor(...)\n",
    "\n",
    "# TODO Trainieren Sie das Modell mit den Trainignsdaten als erstes Argument und den zugehörigen Labels als zweites Argument\n",
    "mlp_regressor.fit(..., ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05781762",
   "metadata": {},
   "source": [
    "### Aufgabe 13\n",
    "\n",
    "Evaluieren Sie nun ihr Modell mit den Testdaten. Dieses Mal können wir aber nicht die Accuracy nutzen, da diese nur für Klassifikationen geeignet ist.\n",
    "Wir nutzen stattdessen den **Mean-Squared-Error (MSE)**. Dieser wird wie folgt berechnet:\n",
    "\n",
    "- $y$: Echtes Label\n",
    "- $\\hat{y}$: Voraussage des Modells\n",
    "\n",
    "$\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2$\n",
    "\n",
    "In Prosa geschieht hier folgendes:\n",
    "Für jedes Data Samples im Testdatenset wird das echte Label minus der Voraussage gerechnet. Dieses Ergebnis wird quadriert. Danach wird die Summe über alle diese quadrierten \"Fehler\" berechnet und geteilt durch die Anzahl Samples gerechnet. Dies ist somit der Mittelwert des quadrierten Fehlers.\n",
    "\n",
    "Vervollständigen Sie den Code um den MSE zu berechnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55863e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Berechne nun mit dem Modell die Vorhersagen für die Testdaten als einzigen Parameter in der predict-Methode\n",
    "y_housing_pred = mlp_regressor.predict(...)\n",
    "\n",
    "# Berechne den Mean Squared Error (MSE) auf dem Testset (Tipp: verwende np.sum und len)\n",
    "mse_test = ...\n",
    "print(f'Mean Squared Error on Test Set: {mse_test:.4f}')\n",
    "\n",
    "assert abs(mse_test - 0.0197) < 0.01, \"Der Mean Squared Error auf dem Testset ist nicht korrekt berechnet.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6102df",
   "metadata": {},
   "source": [
    "### Optional: Aufgabe 14 (Zuerst Aufgabe 10 lösen)\n",
    "Führen Sie die Code-Zelle unten aus. Dabei wird für ein Datasample aus dem Test Dataset der Hauspreis berechnet.\n",
    "\n",
    "Was fällt Ihnen bei dieser Vorhersage auf?\n",
    "\n",
    "\n",
    "\n",
    "Weshalb ist die Vorhersage in dieser Grössenordnung und wie könnten Sie dieses Problem lösen?\n",
    "\n",
    "<br>\n",
    "<details>\n",
    "<summary><b>Tipp 1:</b> Klicke hier für den einen Tipp.</summary>\n",
    "\n",
    "Wir haben auch den House Value mit Min Max Normalisierung skaliert. Wie könnte man dies nun zu einem korrekten Hauswert zurückrechnen?\n",
    "\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1488a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel Vorhersage des Preises für ein einzelnes Haus aus dem Testset\n",
    "\n",
    "y_housing_pred_single = mlp_regressor.predict(X_housing_test[:1])\n",
    "print(f'Der berechnete Hauswert beträgt: {y_housing_pred_single[0]:.2f}')\n",
    "\n",
    "# TODO: Optionale Zusatzaufgabe: Skalieren Sie die Vorhersage zurück in den Originalmassstab. Sie können auf die Originalen min und max Werte der Spalte 'median_house_value' im ursprünglichen DataFrame df_housing zugreifen.\n",
    "y_pred_scaled = ...\n",
    "print(f'Der berechnete Hauswert im Originalmaßstab beträgt: {y_pred_scaled[0]:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54679056",
   "metadata": {},
   "source": [
    "### Aufgabe 15\n",
    "\n",
    "Wir zeigen nun in einem Scatter Plot noch einige zufällige Datenpunkte an, wobei wir vergleichen möchten was der echte Hauspreis ist und was unser Modell berechnet hat.\n",
    "Lassen Sie die nächste Code Zelle laufen und beantworten Sie die folgende Frage.\n",
    "\n",
    "**Frage**\n",
    "Woran erkennt man einen kleinen Fehler des Modells und wie einen grossen?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4bc3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotte die Vorhersagen des Modells gegen die tatsächlichen Werte nutze aber nur 50 zufällige Datenpunkte und zeichne den Fehler als Linie ein\n",
    "\n",
    "random_indices = np.random.choice(len(y_housing_test), size=50, replace=False)\n",
    "y_housing_pred_sampled = y_housing_pred[random_indices]\n",
    "y_housing_test_sampled = y_housing_test.iloc[random_indices]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.scatter(range(len(y_housing_pred_sampled)), y_housing_pred_sampled, color='red', label='Berechnete Werte')\n",
    "plt.scatter(range(len(y_housing_test_sampled)), y_housing_test_sampled, color='blue', label='Tatsächliche Werte')\n",
    "for i in range(len(y_housing_pred_sampled)):\n",
    "    plt.plot([i, i], [y_housing_pred_sampled[i], y_housing_test_sampled.iloc[i]], color='gray', linestyle='--', linewidth=0.5)\n",
    "plt.xlabel('Testdaten Index')\n",
    "plt.ylabel('Median Hauswert (normalisiert)')\n",
    "plt.title('Vorhersagen vs Tatsächliche Werte des Hauswerts')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9126ef",
   "metadata": {},
   "source": [
    "### Zusatzaufgabe: Teste das Training ohne min-max Normalisierung\n",
    "\n",
    "Führe nochmals einen Traingslauf durch ohne, dass die min-max Skalierung genutzt wurde.\n",
    "Beobachte wie lange das Training nun läuft. \n",
    "\n",
    "**Frage**: Was ist schneller? Min-Max normalisierte Daten oder die ursprünglichen Daten?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4eaa366",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0a10b4c",
   "metadata": {},
   "source": [
    "## Kontrollfragen: Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9718682d",
   "metadata": {},
   "source": [
    "**Kontrollfrage 3**\n",
    "\n",
    "Was ist der Output einer Regression und wie verhält sich dieser im Vergleich zu der Klassifikation?\n",
    "\n",
    "\n",
    "\n",
    "**Kontrollfrage 4**\n",
    "\n",
    "Welchen Vorteil hat die Normalisierung der numerischen Features gebracht? Wie lautete die Formel?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "praktikum_ef",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
