{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b9880fe",
   "metadata": {},
   "source": [
    "# pyTorch Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4453fb",
   "metadata": {},
   "source": [
    "<div style=\"padding: 5px; border: 5px solid #a10000ff;\">\n",
    "\n",
    "**Hinweis:** In den Codezellen sind jeweils einige Codeteile nicht programmiert. Diesen Code müssen Sie ergänzen. Die jeweiligen Stellen sind mit einem Kommentar und dem Keyword **TODO** vermerkt und z.T. Stellen mit ... markiert.\n",
    "\n",
    "Ausserdem gibt es einige assert Statements. Diese geben einen Fehler aus, sollte etwas bei Ihrer Programmierung nicht korrekt sein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa9d7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055839ec",
   "metadata": {},
   "source": [
    "## Hintergrundinformationen zu Tensoren und Gradienten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d78f11",
   "metadata": {},
   "source": [
    "Wenn dies auf Ihrem System verfügbar ist, kann PyTorch auch die Grafikkarte (GPU) nutzen um die Berechnungen zu beschleunigen. Ausserdem setzen wir den Random-Number Seed so, dass immer die gleichen Zahlen resultieren. Dies hilft dabei um eine Reproduzierbarkeit zu erreichen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fea290",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "torch.manual_seed(42) # Setze den Zufallsseed für Reproduzierbarkeit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dd7641",
   "metadata": {},
   "source": [
    "### Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af15b7df",
   "metadata": {},
   "source": [
    "Tensoren sind das PyTorch-Äquivalent zu Numpy-Arrays, mit dem zusätzlichen Vorteil der GPU-Beschleunigung (Grafikkarte).\n",
    "Der Name \"Tensor\" ist eine Verallgemeinerung von Konzepten, die Sie bereits kennen. Zum Beispiel ist ein Vektor ein 1-D Tensor und eine Matrix ein 2-D Tensor. Wir werden Tensoren mit verschiedenen Dimensionen verwenden.\n",
    "\n",
    "Die meisten gängigen Funktionen, die Sie aus Numpy kennen, können auch auf Tensoren angewendet werden. Da Numpy-Arrays und Tensoren sich sehr ähneln, können wir die meisten Tensoren in Numpy-Arrays konvertieren und umgekehrt.\n",
    "\n",
    "Der folgende Tensor hat zum Beispiel drei Dimensionen und eine Grösse von 2x3x4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd02454",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor(2, 3, 4)\n",
    "print(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ff46d2",
   "metadata": {},
   "source": [
    "#### Exkurs: Numpy Arrays und PyTorch Tensoren\n",
    "Bei Numpy können Listen verschachtelt werden um eine mehrdimensionale Struktur zu erhalten. Bei PyTorch können wir Tensoren mit verschiedenen Dimensionen erstellen, die ähnlich wie Numpy-Arrays funktionieren. Ein 1D-Tensor ist ein Vektor, ein 2D-Tensor ist eine Matrix, und höhere Dimensionen können als mehrdimensionale Arrays betrachtet werden. Zum Beispiel könnte ein 3D-Tensor als eine Sammlung von 2D-Matrizen betrachtet werden, und so weiter. Die Anzahl der Dimensionen eines Tensors wird als \"Rank\" bezeichnet, und die Grösse jeder Dimension wird als \"Shape\" bezeichnet. Ein Tensor mit der Shape (2, 3, 4) hat zum Beispiel 3 Dimensionen, wobei die erste Dimension 2 Elemente enthält, die zweite Dimension 3 Elemente und die dritte Dimension 4 Elemente enthält."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c042c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "skalar = torch.tensor(1)\n",
    "vektor = torch.tensor([1, 2, 3])\n",
    "matrix = torch.tensor([[1, 2, 3], [3, 4, 5]])\n",
    "tensor_3d = torch.tensor([[[1, 2], [3, 2], [9, 9]], [[1, 7], [5, 4], [3, 7]]])\n",
    "\n",
    "print(f\"Skalar Shape: {skalar.shape}\")\n",
    "print(f\"Vektor Shape: {vektor.shape}\")\n",
    "print(f\"Matrix Shape: {matrix.shape}\")\n",
    "print(f\"3D Tensor Shape: {tensor_3d.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162a7d67",
   "metadata": {},
   "source": [
    "\n",
    "Tensoren können in Numpy-Arrays konvertiert werden, und Numpy-Arrays zurück in Tensoren. Um ein Numpy-Array in einen Tensor umzuwandeln, können wir die Funktion `torch.from_numpy` verwenden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74c5b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_arr = np.array([[1, 2], [3, 4]])\n",
    "tensor = torch.from_numpy(np_arr)\n",
    "\n",
    "print(\"Numpy array:\", np_arr)\n",
    "print(\"PyTorch tensor:\", tensor)\n",
    "\n",
    "np_arr = tensor.numpy()\n",
    "\n",
    "print(\"PyTorch tensor:\", tensor)\n",
    "print(\"Numpy array:\", np_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2fc67d",
   "metadata": {},
   "source": [
    "**Wichtig:** Die Konvertierung von Tensoren zu Numpy erfordert, dass der Tensor auf der CPU und nicht auf der GPU ist. Falls Sie einen Tensor auf der GPU haben, müssen Sie zuerst `.cpu()` auf dem Tensor aufrufen. Daher erhalten Sie eine Zeile wie `np_arr = tensor.cpu().numpy()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8918986",
   "metadata": {},
   "source": [
    "### Automatische Ableitung (Gradient Berechnung)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef16e00",
   "metadata": {},
   "source": [
    "Einer der Hauptgründe für die Verwendung von PyTorch in Deep-Learning-Projekten ist, dass wir automatisch **Gradienten/Ableitungen** erhalten. Wir werden PyTorch zur Implementierung neuronaler Netze verwenden. Wenn wir in unseren Neuronalen Netzwerken Parameter haben, die das Netzwerk lernen soll, werden diese als **Parameter** oder einfach als **Gewichte** bezeichnet. Diese können von Pytorch automatisch gelernt werden, indem die Ableitung bzw. der Gradient berechnet wird und ein Gradient-Descent Verfahren angewendet wird. Dies alles geschieht automatisch, wenn wir die Gewichte als Parameter definieren und die Funktion `backward()` aufrufen. (Siehe Gradient Descent in der ersten Lektion)\n",
    "\n",
    "Manchmal wollen wir aber nicht, dass der Gradient berechnet wird, beziehungsweise dies deaktivieren. Wenn wir zum Beispiel ein Netzwerk evaluieren/testen, können wir somit sicherstellen, dass keien Gradienten berechnet werden. Dies können wir mit `with torch.no_grad()` für einen gesamten Codeblock tun:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b84c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y = x + 2\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dae59a3",
   "metadata": {},
   "source": [
    "### Optional: Hintergrundinfos zum Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6436cc37",
   "metadata": {},
   "source": [
    "Die Klasse `torch.utils.data.DataLoader` stellt einen Python-Iterator über einen Datensatz dar und bietet z.B. automatisches Batching. Der Data Loader kommuniziert mit dem Datensatz über die Funktion `__getitem__` und unterteilt die Daten in Batches, die wir dann für das Training verwenden können. Es werden jeweils eine gegebene Anzahl von Samples zu einem Batch zusammengefasst.\n",
    "\n",
    "Im Gegensatz zur Dataset-Klasse müssen wir normalerweise keine eigene Data-Loader-Klasse definieren, sondern können einfach ein Objekt davon mit dem Datensatz als Eingabe erstellen. Zusätzlich können wir unseren Data Loader mit den folgenden Eingabeargumenten konfigurieren (nur eine Auswahl, vollständige Liste [hier](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)):\n",
    "\n",
    "* `batch_size`: Anzahl der Samples pro Batch\n",
    "* `shuffle`: Falls True, werden die Daten in zufälliger Reihenfolge zurückgegeben. Dies ist wichtig während des Trainings zur Einführung von Stochastizität.\n",
    "* `num_workers`: Anzahl der Subprozesse für das Datenladen. Der Standard 0 bedeutet, dass Daten im Hauptprozess geladen werden, was das Training bei Datensätzen mit langen Ladezeiten verlangsamen kann (z.B. grosse Bilder). Mehr Worker werden empfohlen, können aber auf Windows-Computern Probleme verursachen. Bei kleinen Datensätzen wie unserem ist 0 Worker normalerweise schneller.\n",
    "* `drop_last`: Falls True, wird der letzte Batch verworfen, falls er kleiner als die spezifizierte Batch-Grösse ist. Dies tritt auf, wenn die Datensatzgrösse nicht ein Vielfaches der Batch-Grösse ist. Nur möglicherweise hilfreich während des Trainings zur Konsistenz der Batch-Grösse.\n",
    "\n",
    "Unten erstellen wir einen einfachen Data Loader mit einem gegebenen Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabd8e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wir erstellen einen Regressionsdatensatz aus sklearn in einen DataLoader von pytorch\n",
    "from sklearn.datasets import make_regression\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "dataloader = DataLoader(torch.utils.data.TensorDataset(torch.tensor(X, dtype=torch.float32),\n",
    "                                                        torch.tensor(y, dtype=torch.float32)), batch_size=10)\n",
    "\n",
    "for batch_X, batch_y in dataloader:\n",
    "    print(batch_X.shape, batch_y.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71de3f1c",
   "metadata": {},
   "source": [
    "## Aufgaben Beispiel Netzwerk trainieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7941fa06",
   "metadata": {},
   "source": [
    "Wir betrachten ein Dataset, welches 11 Eigenschaften von Weinen enthält. Es ist nun die Aufgabe eines Machine Learning Modells die Weinqualität vorherzusagen. In diesem Notebook werden wir ein einfaches neuronales Netzwerk mit einer versteckten Schicht implementieren und trainieren. Das Ziel ist es, die Grundlagen von PyTorch zu verstehen, einschließlich der Erstellung von Modellen, der Definition von Lossfunktionen und der Durchführung von Trainingsschritten.\n",
    "\n",
    "Das Dataset hat folgende Features (Merkmale der Weine):\n",
    "1. **fixed acidity**: Die Menge an festen Säuren im Wein, gemessen in Gramm pro Liter.\n",
    "2. **volatile acidity**: Die Menge an flüchtigen Säuren im Wein, gemessen in Gramm pro Liter. \n",
    "3. **citric acid**: Die Menge an Zitronensäure im Wein, gemessen in Gramm pro Liter.\n",
    "4. **residual sugar**: Die Menge an Restzucker im Wein, gemessen in Gramm pro Liter. \n",
    "5. **chlorides**: Die Menge an Chloriden im Wein, gemessen in Gramm pro Liter\n",
    "6. **free sulfur dioxide**: Die Menge an freiem Schwefeldioxid im Wein, gemessen in Milligramm pro Liter\n",
    "7. **total sulfur dioxide**: Die Gesamtmenge an Schwefeldioxid im Wein, gemessen in Milligramm pro Liter. \n",
    "8. **density**: Die Dichte des Weins, gemessen in Gramm pro Kubikzentimeter.\n",
    "9. **pH**: Der pH-Wert des Weins.\n",
    "10. **sulphates**: Die Menge an Sulfaten im Wein, gemessen in Gramm pro Liter.\n",
    "11. **alcohol**: Der Alkoholgehalt des Weins, gemessen in Volumenprozent.\n",
    "\n",
    "Quality ist die Zielvariable (Target), die die Qualität des Weins auf einer Skala von 0 bis 10 bewertet. In diesem Notebook werden wir versuchen, diese Qualität basierend auf den 11 Eigenschaften vorherzusagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761ab181",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_dataframe = pd.read_csv(\"datasets/wineQT.csv\", index_col=\"Id\")\n",
    "wine_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17a91ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wir splitten die Daten in Trainings- und Testdaten\n",
    "train_df, test_df = train_test_split(wine_dataframe, test_size=0.2, random_state=42)\n",
    "print(f\"Train shape: {train_df.shape}, Test shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfe07aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wir erstellen einen DataLoader für die Trainingsdaten\n",
    "# Der hilft uns, dass wir die Daten in Batches also in Stücken durch das Netzwerk passieren können ohne alles auf einmal laden zu müssen\n",
    "# Dies macht das Training effizienter und spart Speicherplatz\n",
    "\n",
    "train_loader = DataLoader(torch.utils.data.TensorDataset(\n",
    "    torch.tensor(train_df.drop(\"quality\", axis=1).values, dtype=torch.float32),\n",
    "    torch.tensor(train_df[\"quality\"].values, dtype=torch.float32)\n",
    "), batch_size=16, shuffle=True, drop_last=True)\n",
    "\n",
    "for batch_X, batch_y in train_loader:\n",
    "    print(batch_X.shape, batch_y.shape)\n",
    "    break\n",
    "\n",
    "# Wir erstellen einen DataLoader für die Testdaten\n",
    "test_loader = DataLoader(torch.utils.data.TensorDataset(\n",
    "    torch.tensor(test_df.drop(\"quality\", axis=1).values, dtype=torch.float32),\n",
    "    torch.tensor(test_df[\"quality\"].values, dtype=torch.float32)\n",
    "), batch_size=16, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2ac2c5",
   "metadata": {},
   "source": [
    "### Aufgabe 1 Traingsloop implementieren\n",
    "\n",
    "In der folgenden Zelle soll eine Funktion erstellt werden womit ein Trainingsloop implementiert werden. Dieser soll für eine gegebene Anzahl Epochen über die Trainingsdaten iterieren, die Vorhersagen des Modells berechnen, den Verlust (MSELoss) bestimmen, den Gradienten berechnen und die Modellparameter mit dem Adam-Optimizer updaten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8158190d",
   "metadata": {},
   "outputs": [],
   "source": [
    " # TODO Vervollständigen Sie die Funktion train_model unten\n",
    "def train_model(model, train_loader, loss_function, optimizer, num_epochs):\n",
    "    trainlosses = []\n",
    "    # testlosses = [] # Kann optional verwendet werden, um den Testloss zu verfolgen\n",
    "    for epoch in range(num_epochs):\n",
    "        batch_losses = []\n",
    "        for batch_inputs, batch_targets in train_loader:\n",
    "            #Wir verschieben die Daten auf das gleiche Gerät wie das Modell (CPU oder GPU)\n",
    "            batch_inputs, batch_targets = batch_inputs.to(device), batch_targets.to(device)\n",
    "\n",
    "            # Vorhersagen berechnen\n",
    "            predictions = ...\n",
    "            \n",
    "            # Verlust berechnen\n",
    "            loss = ...\n",
    "            \n",
    "            # Gradienten zurücksetzen\n",
    "            ...\n",
    "            \n",
    "            # Gradienten berechnen\n",
    "            ...\n",
    "            \n",
    "            # Modellparameter updaten\n",
    "            ...\n",
    "            \n",
    "            batch_losses.append(loss.item())\n",
    "        # testlosses.append(test_model(model, test_loader, loss_function)) # Kann optional verwendet werden, um den Testloss zu verfolgen\n",
    "        \n",
    "        trainlosses.append(np.mean(batch_losses))\n",
    "\n",
    "    return trainlosses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b518939",
   "metadata": {},
   "source": [
    "### Aufgabe 2: Model testen (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdf51c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Vervollständigen Sie die Funktion test_model, die das Modell evaluiert. \n",
    "# Diese macht ähnliche Schritte wie die train_model Funktion, aber ohne Gradientenberechnung und Parameter-Updates.\n",
    "# Wir können dazu mit torch.no_grad(): den Gradientenmodus ausschalten und Speicher sparen.\n",
    "\n",
    "def test_model(model, test_loader, loss_function):\n",
    "    with torch.no_grad():\n",
    "        test_losses = []\n",
    "        for batch_inputs, batch_targets in test_loader:\n",
    "            batch_inputs, batch_targets = batch_inputs.to(device), batch_targets.to(device)\n",
    "            # Vorhersagen berechnen\n",
    "            predictions = ...\n",
    "            \n",
    "            # Verlust berechnen\n",
    "            loss = ...\n",
    "            test_losses.append(loss.item())\n",
    "        \n",
    "        average_loss = sum(test_losses) / len(test_losses)\n",
    "        #print(f\"Average test loss: {average_loss}\")\n",
    "        return average_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad096c2b",
   "metadata": {},
   "source": [
    "### Aufgabe 3: Model erstellen und trainieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571da5ba",
   "metadata": {},
   "source": [
    "Erstellen Sie ein Model mit einer versteckten Schicht mit 50 Neuronen und ReLU Aktivierungsfunktion. Sie können im Notebook **00_Training_Learning** unter dem Titel **Modell erstellen und trainieren** nachschauen, wie man ein Modell erstellt. \n",
    "\n",
    "Trainieren Sie das Model anschliessend mit dem Trainingsloop aus Aufgabe 1 für 100 Epochen. \n",
    "Verwenden Sie beim Adam Optimizer eine Lernrate von 0.001 und beim Training eine Batch-Grösse von 32. Geben Sie danach den finalen Trainingsloss aus.\n",
    "\n",
    "Das Dataset hat 11 Input-Features und 1 Target (Weinqualität). Deshalb benötigt das Model 11 Input-Neuronen und 1 Output-Neuron. Das Model soll direkt den Weinqualität-Wert vorhersagen (Regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68864fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Modell erstellen mit einer versteckten Schicht mit 50 Neuronen und ReLU Aktivierungsfunktion\n",
    "# Verwenden Sie wie im ersten Beispiel nn.Sequential mit Linear Layers und einer ReLU Aktivierungsfunktion dazwischen\n",
    "model = torch.nn.Sequential(\n",
    "...\n",
    ")\n",
    "# Modell auf das richtige Gerät verschieben (CPU oder GPU)\n",
    "model.to(device)\n",
    "\n",
    "# Verlustfunktion und Optimizer definieren\n",
    "loss_function = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# TODO Trainieren Sie das Model mit dem Trainingsloop aus Aufgabe 1 für 100 Epochen\n",
    "num_epochs = 100\n",
    "train_losses = ...\n",
    "print(f\"Final training loss: {train_losses[-1]}\")\n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05fedb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Testen Sie das Modell mit der test_model Funktion und geben Sie den finalen Testloss aus\n",
    "# average_test_loss = test_model(model, test_loader, loss_function)\n",
    "\n",
    "# print(f\"Final test loss: {average_test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51b65ec",
   "metadata": {},
   "source": [
    "### Aufgabe 4: Modell speichern und laden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ef9da4",
   "metadata": {},
   "source": [
    "Das Training des Modells dauert einige Sekunden. Bei grösseren Modellen und Datensätzen kann das Training Minuten, Stunden oder Tage dauern. Um das trainierte Modell später wiederverwenden zu können, ohne es erneut trainieren zu müssen, können wir die Modellparameter speichern und später wieder laden.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa264a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO speichern und laden Sie das trainierte Modell mit torch.save und torch.load\n",
    "\n",
    "#Speichern des Modells\n",
    "torch.save(model.state_dict(), \"trained_model.pth\")\n",
    "# Es werden nur die Modellparameter gespeichert, nicht die gesamte Modellarchitektur. \n",
    "# Deshalb müssen wird das Modell erneut definieren bevor wir die Parameter laden können.\n",
    "\n",
    "loaded_model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(11, 50),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(50, 1)\n",
    ")\n",
    "loaded_model.load_state_dict(torch.load(\"trained_model.pth\"))\n",
    "loaded_model.to(device)\n",
    "\n",
    "# Wir testen das Modell nochmals nach dem Laden\n",
    "loaded_model.eval()\n",
    "average_test_loss_loaded = test_model(loaded_model, test_loader, loss_function)\n",
    "\n",
    "## Erscheint der gleiche Testloss wie in Aufgabe 3?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e71626",
   "metadata": {},
   "source": [
    "### Aufgabe 5: Optional: Modell-Trainingsverlauf visualisieren\n",
    "\n",
    "Plotten Sie den Trainingsverlust über die Epochen um den Trainingsverlauf zu visualisieren.\n",
    "\n",
    "**Frage:** Hat es Sinn gemacht, das Modell über 100 Epochen zu trainieren? Oder hätte es auch mit weniger Epochen gereicht?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abf838d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(1, num_epochs + 1), train_losses, label=\"Trainingsloss\")\n",
    "ax.set_xlabel(\"Epoche\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_title(\"Trainingsverlauf\")\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0820d669",
   "metadata": {},
   "source": [
    "### Aufgabe 6: Optional: Modell ändern\n",
    "\n",
    "In der vorherigen Aufgabe haben Sie ein einfaches Feedforward-Netzwerk mit einer versteckten Schicht implementiert. Versuchen Sie nun, die Modellarchitektur zu ändern. Sie können z.B. weitere versteckte Schichten hinzufügen, die Anzahl der Neuronen pro Schicht ändern oder andere Aktivierungsfunktionen ausprobieren. Trainieren Sie das veränderte Modell erneut und vergleichen Sie die Testergebnisse mit dem ursprünglichen Modell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88060eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Verbessertes Modell erstellen und testen\n",
    "improved_model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(11, 50),\n",
    "    torch.nn.LogSigmoid(),\n",
    "    torch.nn.Linear(50, 50),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(50, 1)\n",
    ")\n",
    "improved_model.to(device)\n",
    "improved_optimizer = torch.optim.Adam(improved_model.parameters(), lr=0.001)\n",
    "improved_train_losses = train_model(improved_model, train_loader, loss_function, improved_optimizer, num_epochs)\n",
    "average_test_loss_improved = test_model(improved_model, test_loader, loss_function)\n",
    "print(f\"Final training loss of improved model: {improved_train_losses[-1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "praktikum_ef",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
