{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b9880fe",
   "metadata": {},
   "source": [
    "# Training, Underfitting & Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4453fb",
   "metadata": {},
   "source": [
    "<div style=\"padding: 5px; border: 5px solid #a10000ff;\">\n",
    "\n",
    "**Hinweis:** In den Codezellen sind jeweils einige Codeteile nicht programmiert. Diesen Code müssen Sie ergänzen. Die jeweiligen Stellen sind mit einem Kommentar und dem Keyword **TODO** vermerkt und z.T. Stellen mit ... markiert.\n",
    "\n",
    "Ausserdem gibt es einige assert Statements. Diese geben einen Fehler aus, sollte etwas bei Ihrer Programmierung nicht korrekt sein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda0f5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e33c16",
   "metadata": {},
   "source": [
    "## Training von Netzwerken, Under-/Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce259a2",
   "metadata": {},
   "source": [
    "Wir haben bereits besprochen, dass ein Machine Learning Modell generalisiert.\n",
    "In diesem Notebook möchten wir einen Effekt betrachten, welcher auftreten kann, wenn ein Modell die Daten zu schlecht oder zu gut lernt.\n",
    "\n",
    "Wenn ein Modell die Daten kaum lernt also nicht gut auf die Trainingsdaten fitted spricht man von **Underfitting**.\n",
    "\n",
    "Wenn ein Modell die Trainingsdaten jedoch perfekt abbildet und auf Testdaten deutlich schlechter funktioniert spricht man von **Overfitting**.\n",
    "\n",
    "\n",
    "Führen Sie die Zelle unten aus und beantworten danach die Fragen unter der Zelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68db8189",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(-3, 3, 30)\n",
    "y = np.array([6.40320211e-01, 4.65454802e-01, 3.18542423e-01, 2.41632279e-01,\n",
    "  7.90939261e-02, -3.68292645e-02,  8.43445053e-02,  7.45484173e-03,\n",
    " -2.57748336e-01, -3.88458508e-05,  2.23775450e-02,  9.56693324e-02,\n",
    "  6.59981685e-02, -1.18768602e-01, -3.90238869e-02,  2.32054080e-01,\n",
    "  2.70815440e-01, -8.54839118e-03, -5.38726619e-02,  8.32862909e-02,\n",
    "  3.93309346e-01,  8.69511514e-02,  3.50274734e-01, -1.69659711e-01,\n",
    "  1.97757116e-02, -4.07498944e-01, -2.18885263e-01, -3.86262075e-01,\n",
    " -4.67668757e-01, -7.57073736e-01])\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X = X.reshape(-1, 1)\n",
    "y = y.reshape(-1, 1)\n",
    "X_train = X_train.reshape(-1, 1)\n",
    "X_test = X_test.reshape(-1, 1)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.FloatTensor(y_train)\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_tensor = torch.FloatTensor(y_test)\n",
    "\n",
    "# Drei verschiedene Modelle\n",
    "model1 = nn.Sequential(\n",
    "    nn.Linear(1, 12),\n",
    "    nn.LogSigmoid(),\n",
    "    nn.Linear(12, 1)\n",
    ")\n",
    "\n",
    "model2 = nn.Sequential(\n",
    "    nn.Linear(1, 3),\n",
    "    nn.Identity(),\n",
    "    nn.Linear(3, 1)\n",
    ")\n",
    "model3 = nn.Sequential(\n",
    "    nn.Linear(1, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 1)\n",
    ")\n",
    "models = [model1, model2, model3]\n",
    "\n",
    "# Training Funktion\n",
    "def train_model(model, X_train, y_train, X_test, y_test, num_epochs=3000, learning_rate=0.01):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_outputs = model(X_test)\n",
    "            test_loss = criterion(test_outputs, y_test)\n",
    "            test_losses.append(test_loss.item())\n",
    "    return train_losses, test_losses\n",
    "\n",
    "# Model Training und Visualisierung\n",
    "for i, model in enumerate(models):\n",
    "    train_losses, test_losses = train_model(model, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(len(train_losses)), train_losses, label='Train Loss')\n",
    "    plt.plot(range(len(test_losses)), test_losses, label='Test Loss')\n",
    "    plt.title(f'Model {i+1} Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.ylim(0, 0.2)\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(torch.FloatTensor(X))\n",
    "    plt.scatter(X_train, y_train, label='Train Data', color='blue', alpha=0.5)\n",
    "    plt.scatter(X_test, y_test, label='Test Data', color='green', alpha=0.5)\n",
    "    plt.plot(X, predictions.numpy(), color='red', label='Model Prediction')\n",
    "    plt.title(f'Model {i+1} Predictions')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d506ef9",
   "metadata": {},
   "source": [
    "## Aufgabe 1\n",
    "\n",
    "**Frage A:** Betrachten Sie das Modell 2. Was beobachten Sie bezüglich des absoluten Loss für Training und Testing?\n",
    "\n",
    "**Frage B:** Betrachten Sie das Modell 3. Was fällt Ihnen bezüglich des Train und Test Loss auf? \n",
    "\n",
    "**Frage C:** Welches Modell findet die beste Balance zwischen Flexibilität und Generalisierung?\n",
    "\n",
    "**Frage D:** Welches Modell ist von Overfitting bzw. Underfitting betroffen?\n",
    "\n",
    "**Frage E:** Ist es sinnvoll immer einen möglichst kleinen Training-Loss anzustreben?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "praktikum_ef",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
