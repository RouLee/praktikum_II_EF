{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b9880fe",
   "metadata": {},
   "source": [
    "# Komponenten eines neuronalen Netzwerkes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4453fb",
   "metadata": {},
   "source": [
    "<div style=\"padding: 5px; border: 5px solid #a10000ff;\">\n",
    "\n",
    "**Hinweis:** In den Codezellen sind jeweils einige Codeteile nicht programmiert. Diesen Code müssen Sie ergänzen. Die jeweiligen Stellen sind mit einem Kommentar und dem Keyword **TODO** vermerkt und z.T. Stellen mit ... markiert.\n",
    "\n",
    "Ausserdem gibt es einige assert Statements. Diese geben einen Fehler aus, sollte etwas bei Ihrer Programmierung nicht korrekt sein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda0f5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73722808",
   "metadata": {},
   "source": [
    "## Perzeptronen\n",
    "\n",
    "Perzeptronen sind künstliche Neuronen, welche eine Anzahl von numerischen Inputs entgegennehmen, diese mit einem Gewicht multiplizieren und danach eine Summe daraus bilden. Die Aktivierungsfunktion des Perzeptrons gibt, danach einen Wert aus je nach Grösse der Summe. In der folgenden Darstellung wird dies grafisch dargestellt.\n",
    "\n",
    "![Perzeptron Skizze](images/perzeptron_1.png)\n",
    "\n",
    "\n",
    "\n",
    "**Gewichte**\n",
    "\n",
    "Die Gewichte definieren, wie stark der Input gewichtet wird und somit den Output des Perzeptrons beeinflusst.\n",
    "\n",
    "**Bias**\n",
    "\n",
    "Jedes Perzeptron hat noch einen Bias, welcher ein Gewicht darstellt, welches unabhängig vom Input ist und somit die Aktivierung unabhängig vom den Inputs steuern kann.\n",
    "\n",
    "**Aktivierungsfunktion**\n",
    "\n",
    "Für die Aktivierungsfunktion gibt es verschiedene Varianten. In dem Beispiel oben wird die `sign()` Funktion genutzt.\n",
    "Weitere Aktivierungsfunktionen sind zum Beispiel:\n",
    "\n",
    "- $\\text{Linear:} f(z)=z $\n",
    "- $\\text{Logistic (Sigmoid): } f(z)=\\frac{1}{1+e^{-z}}$\n",
    "- $\\text{LeakyReLU: } f(z) =\n",
    "\\begin{cases}\n",
    "z, & \\text{if } z \\ge 0 \\\\\n",
    "\\alpha x, & \\text{if } z < 0\n",
    "\\end{cases}$\n",
    "\n",
    "**Optional**:\n",
    "Auf folgender Seite finden Sie Visualisierungen zu den Aktivierungsfunktionen:\n",
    "https://www.geeksforgeeks.org/machine-learning/activation-functions-neural-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a8e851",
   "metadata": {},
   "source": [
    "### Beispielberechnung\n",
    "\n",
    "Gegeben ein Perzeptron hat zwei Inputs und somit zwei Gewichten und einem Bias. Genutzt wird die `sign()` Aktivierungsfunktion.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17093e8",
   "metadata": {},
   "source": [
    "Für folgende Konfiguration und Inputs:\n",
    "\n",
    "$x_1= 5$\n",
    "$x_2= -2$\n",
    "\n",
    "$w_0 (b) = -6$\n",
    "$w_1 = 1$\n",
    "$w_2 = -4$\n",
    "\n",
    "Gewichtete Summe berechnen:\n",
    "\n",
    "$ z =x_1 \\cdot w_1 + x_2 \\cdot w_2 + b = 5 \\cdot 1 + (-2) \\cdot (-4) + (-6) = 7$\n",
    "\n",
    "Aktivierungsfunktion anwenden:\n",
    "\n",
    "$ f(z) =\\begin{cases}\n",
    "1, & \\text{if } z \\ge 0 \\\\\n",
    "-1, & \\text{if } z < 0\n",
    "\\end{cases} = 1$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05588000",
   "metadata": {},
   "source": [
    "### Aufgabe 1\n",
    "Schreiben Sie eine Funktion, welche ein simples Perzeptron mit zwei Inputs, zwei Gewichten und der Sign activation Funktion abbildet. Vervollständigen Sie dazu den folgenden Code. Die \"*assert*\" Statements generieren einen Fehler, sollte die Implementation nicht korrekt sein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e09dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_function(z):\n",
    "    # TODO Implementieren Sie die sign-Aktivierungsfunktion\n",
    "    \n",
    "    return None\n",
    "\n",
    "def perceptron(w1, w2, bias, x1, x2):\n",
    "    # TODO Implementieren Sie den Output des Perzeptrons mit der gewichteten Summe und der Aktivierungsfunktion\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "assert activation_function(3) == 1, \"Testfall 1 für Aktivierungsfunktion fehlgeschlagen\"\n",
    "assert activation_function(-2) == -1, \"Testfall 2 für Aktivierungsfunktion fehlgeschlagen\"\n",
    "assert activation_function(0) == 1, \"Testfall 3 für Aktivierungsfunktion fehlgeschlagen\"\n",
    "assert perceptron(1, -4, -6, 5, -2) == 1, \"Testfall 1 für Perzeptron fehlgeschlagen\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d11ddb",
   "metadata": {},
   "source": [
    "### Lernalgorithmus Perzeptron\n",
    "\n",
    "**Optional:**\n",
    "\n",
    "Die Gewichte haben wir hier als gegeben genommen. In der Realität würden diese jedoch von Trainingsdaten gelernt werden. Wir schauen uns in diesem Kurs den Lernalgorithmus des Perzeptrons nicht an, da dies in heutigen neuronalen Netzerken nicht mehr anhand des alten Algorithmus geschieht. Sollten Sie trotzdem daran interessiert sein, können Sie hier unter Working genauer nachlesen, wie nun die Gewichte aktualisiert werden: https://www.geeksforgeeks.org/machine-learning/what-is-perceptron-the-simplest-artificial-neural-network/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589bee1e",
   "metadata": {},
   "source": [
    "## Verschiedene Netzwerke vergleichen\n",
    "\n",
    "Wir möchten uns nun anschauen wie sich unterschiedliche Netzwerke anhand eines Klassifikationsproblems verhalten. Dabei achten wir vor allem auf die Bildung der Decision Boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412b9f3b",
   "metadata": {},
   "source": [
    "### Single Perzeptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b3e648",
   "metadata": {},
   "source": [
    "Wir betrachten nun einmal ein künstliches Klassifikationsproblem. Dabei sollen die Datenpunkte in zwei unterschiedliche Klassen unterteilt werden. Dazu wird ein neues Dataset generiert mit 100 Datenpunkten. Je 50 Datenpunkte pro Klasse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08734ae8",
   "metadata": {},
   "source": [
    "### Dataset generieren und visualisieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20215511",
   "metadata": {},
   "source": [
    "Die folgende Funktion erstellt ein Dataset, welches linear separierbar ist. Das bedeutet, es kann durch eine lineare Decision Boundary fehlerlos unterteilt werden. Lassen Sie die folgende Zelle laufen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b79208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a linearly separable dataset with two features and two classes\n",
    "def create_linearly_separable_data(num_samples_per_class=50):\n",
    "    np.random.seed(0)  # for reproducibility\n",
    "    class_0 = np.random.randn(num_samples_per_class, 2) + np.array([-2, -2])\n",
    "    class_1 = np.random.randn(num_samples_per_class, 2) + np.array([2, 2])\n",
    "    data = np.vstack((class_0, class_1))\n",
    "    labels = np.array([0]*num_samples_per_class + [1]*num_samples_per_class)\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058fec79",
   "metadata": {},
   "source": [
    "### Aufgabe 2\n",
    "\n",
    "Vervollständigen Sie den Code und visualisieren Sie die Daten mit matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf1915e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Visualisieren Sie die Datenpunkte in einem Scatter-Plot\n",
    "data, labels = create_linearly_separable_data()\n",
    "fig, ax = plt.subplots()\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54caf049",
   "metadata": {},
   "source": [
    "### Aufgabe 3\n",
    "\n",
    "In der folgenden Zelle wurde das Perzeptron mittels Vektoroperationen programmiert. Lassen Sie die Zelle laufen und beantworten Sie folgende Frage.\n",
    "\n",
    "**Frage**\n",
    "\n",
    "Welche Aktivierungsfunktion nutzt das Perzeptron?\n",
    "> Es wird eine Step Function bentutzt die 0 ausgibt wenn die gewichtete Summe unter Null ist und ansonsten 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349d1131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perceptron function using vectorized operations\n",
    "def perceptron_step(weight_vector, bias, input_vector):\n",
    "    weighted_sum = np.dot(weight_vector, input_vector) + bias\n",
    "    return 1 if weighted_sum >= 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002e0a04",
   "metadata": {},
   "source": [
    "### Lernalgorithmus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8d7fba",
   "metadata": {},
   "source": [
    "In den folgenden Zelle ist der Lernalgorithmus und die Visualisierung des Datasets inklusive Decision Boundary für ein Perzeptron implementiert. Lassen Sie die Zelle laufen damit die Funktionen registriert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0e56d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lernalgorithmus für das Perzeptron\n",
    "# X: input data, y: labels, learning_rate: step size, epochs: number of iterations\n",
    "def train_perceptron(X, y, learning_rate=0.1, epochs=10):\n",
    "    num_features = X.shape[1]\n",
    "    weights = np.zeros(num_features)\n",
    "    bias = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(X.shape[0]):\n",
    "            prediction = perceptron_step(weights, bias, X[i])\n",
    "            error = y[i] - prediction\n",
    "            weights += learning_rate * error * X[i]\n",
    "            bias += learning_rate * error\n",
    "    return weights, bias\n",
    "\n",
    "#plot in a scatter plot the data points with different colors for the two classes\n",
    "def plot_data_with_perc(X, y, weights=None, bias=None, title='Linearly Separable Data'):\n",
    "    if weights is not None and bias is not None:\n",
    "        # create a grid to plot the decision boundary\n",
    "        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "        Z = np.array([perceptron_step(weights, bias, np.array([x, y])) for x, y in zip(xx.ravel(), yy.ravel())])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        plt.contourf(xx, yy, Z, alpha=0.3, cmap='bwr')\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr', edgecolors='k')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.show()\n",
    "\n",
    "# evaluate the perceptron on the test set\n",
    "def evaluate_perceptron(X, y, weights, bias):\n",
    "    correct_predictions = 0\n",
    "    for i in range(X.shape[0]):\n",
    "        prediction = perceptron_step(weights, bias, X[i])\n",
    "        if prediction == y[i]:\n",
    "            correct_predictions += 1\n",
    "    accuracy = correct_predictions / X.shape[0]\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a8a92f",
   "metadata": {},
   "source": [
    "### Aufgabe 4:\n",
    "Vervollständigen Sie den Code um ein Dataset sich generieren zu lassen, die Dimensionen der Testdaten anzuzeigen (Tipp: nutzen Sie die shape Eigenschaft) und dieses in ein Trainings- und Testdatenset zu unterteilen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845774f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Rufen Sie die Funktion create_linearly_separable_data auf und geben Sie die Dimensionen der zurückgegebenen Arrays aus\n",
    "\n",
    "X_1, y_1 = create_linearly_separable_data()\n",
    "\n",
    "print(\"Dimensionen der Datenpunkte X_1:\", X_1.shape)\n",
    "print(\"Dimensionen der Labels y_1:\", y_1.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a1d42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Vervollständigen Sie den Code. Unterteilen Sie Daten in Trainings- und Testdaten mit 20% Testdaten\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(...)\n",
    "\n",
    "# Belassen Sie den Code unten unverändert.\n",
    "weights, bias = train_perceptron(X_train, y_train, learning_rate=0.1, epochs=20)\n",
    "accuracy = evaluate_perceptron(X_test, y_test, weights, bias)\n",
    "print(\"Test set accuracy:\", accuracy)\n",
    "plot_data_with_perc(X_1, y_1, weights, bias)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb37ed4",
   "metadata": {},
   "source": [
    "**Bemerkung:** Die Decision Boundary wäre eigentlich linear und gerade, wird jedoch aufgrund der Methode wie Sie plotted wird fälschlicherweise mit kleinen Stufen dargestellt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a0d65d",
   "metadata": {},
   "source": [
    "## Multilayer Perzeptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bc2ff4",
   "metadata": {},
   "source": [
    "Wir haben gesehen, dass das Perzeptron eine gerade Decision Boundary lernt. In heutigen neuronalen Netzwerken kommen sehr viele Perzeptronen oder künstliche Neuronen zum Einsatz. Die Perzeptronen haben dabei gleich wie vorhin behandelt Gewichte, einen Bias, die gewichtete Summe und eine Aktivierungsfunktion. Die Perzeptronen in Layern also Schichten hintereinander geschaltet. Normalerweise sind die Neuronen von zwei angrenzenden Schichten alle miteinander je verbunden.\n",
    "Beispiel:\n",
    "\n",
    "![Multi-Layer Perceptron](images/mlp1.png) \n",
    "\n",
    "*Quelle: ibm.com*\n",
    "\n",
    "Dabei hat jedes Perzeptron selbst so viele Gewichte wie Verbindungen bei ihm eingehen. Zum Beispiel wird dann der Output des Perceptrons P1 beim Perzeptrons P3 mit einem Gewicht multipliziert.\n",
    "\n",
    "![Beispiel drei Perzeptronen](images/mlp2.png)\n",
    "\n",
    "In der nächsten Aufgabe sollen Sie unterschiedliche Aktivierungsfunktionen testen und deren Auswirkung beobachten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b13e517",
   "metadata": {},
   "source": [
    "### Aufgabe 5\n",
    "In der folgenden Zelle ist ein MLP Klassifikationsmodell aus der Library Scikit-Learn implementiert und auch gleich die Visualisierung dargestellt. Sie müssen den Code nicht mehr ändern.\n",
    "\n",
    "**Frage**\n",
    "Halten Sie fest, wie sich die Decision Boundary ändert, wenn Sie die Aktivierungsfunktion anpassen:\n",
    "\n",
    "> identity: gerade\n",
    "> relu: stark gebogen\n",
    "> logistic: leicht gebogen\n",
    "> tanh: S-Form\n",
    "\n",
    "- identity:\n",
    "- relu:\n",
    "- logistic:\n",
    "- tanh:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5251048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a Multilayer Perceptron to demonstrate non-linearity\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(4,), activation='relu', max_iter=2000, random_state=41)\n",
    "mlp.fit(X_train, y_train)\n",
    "mlp_accuracy = mlp.score(X_test, y_test)\n",
    "print(\"MLP Test set accuracy:\", mlp_accuracy)\n",
    "\n",
    "# plot decision boundary for MLP\n",
    "def plot_mlp_decision_boundary(mlp, X, y):\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "    Z = mlp.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='bwr')\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr', edgecolors='k')\n",
    "    plt.title('MLP Decision Boundary')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.show()\n",
    "\n",
    "plot_mlp_decision_boundary(mlp, X_1, y_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce2caa0",
   "metadata": {},
   "source": [
    "## XOR-Problem\n",
    "Wir haben gesehen, dass die Aktivierungsfunktion einen grossen Einfluss auf den Output haben kann.\n",
    "\n",
    "In den 1960er Jahren herrschte grosser Optimismus gegenüber Künstlicher Intelligenz. Ein zentrales Modell dieser Zeit war das Perzeptron, ein einfacher neuronaler Klassifikator. Dieser konnte jedoch nur linear separierbare Probleme lösen.\n",
    "\n",
    "Das XOR-Problem (Exklusives Oder) zeigte eine fundamentale Schwäche dieses Ansatzes: Die XOR-Funktion ist nicht linear separierbar. Das bedeutet, dass kein einzelnes Perzeptron eine Entscheidungsgrenze finden kann, um XOR korrekt zu klassifizieren.\n",
    "Zur Erinnerung, XOR hat folgende Wahrheisttabelle:\n",
    "\n",
    "| A | B | A XOR B |\n",
    "|---|---|---------|\n",
    "| 0 | 0 |    0    |\n",
    "| 0 | 1 |    1    |\n",
    "| 1 | 0 |    1    |\n",
    "| 1 | 1 |    0    |\n",
    "\n",
    "Wenn Sie die folgende Code-Zelle laufen lassen, sehen Sie noch eine Grafik des XOR-Problems und dass es keine lineare decision Boundary gibt, welche die Klassen fehlerlos trennt.\n",
    "\n",
    "1969 wiesen Marvin Minsky und Seymour Papert in ihrem Buch “Perceptrons” nach, dass Perzeptronen diese Fähigkeit fehlt. Da zu dieser Zeit mehrschichtige neuronale Netze und effektive Trainingsmethoden wie Backpropagation noch nicht etabliert waren, wurde daraus der (falsche) Schluss gezogen, dass neuronale Netze grundsätzlich stark limitiert seien.\n",
    "\n",
    "Diese Erkenntnis führte zu massivem Vertrauensverlust, gekürzten Forschungsgeldern und stagnierender Entwicklung im Bereich neuronaler Netze – eine Phase, die heute als AI Winter bezeichnet wird.\n",
    "\n",
    "Erst in den 1980er Jahren, mit der Wiederentdeckung von mehrschichtigen Netzen und Backpropagation, konnte das XOR-Problem erfolgreich gelöst werden, was den Grundstein für das moderne Deep Learning legte.\n",
    "\n",
    "Wir möchten nun das XOR Problem betrachten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772e6a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xor_x, xor_y = np.array([[0,0],[0,1],[1,0],[1,1]]), np.array([0,1,1,0])\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(xor_x[:,0], xor_x[:,1], c=xor_y, cmap='bwr', edgecolors='k')\n",
    "ax.set_title('XOR Data')\n",
    "ax.set_xlabel('A')\n",
    "ax.set_ylabel('B')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bf9f48",
   "metadata": {},
   "source": [
    "Wir interpretieren nun die obigen Werte für A und B als zwei Input Features und den Wert A XOR B als Label. \n",
    "Nun möchten wir untersuchen ob und mit welcher Aktivierungsfunktion, MLPs dieses Problem lösen können. Dazu generieren wir einige Datenpunkte im XOR Muster, damit das Modell genug Daten hat um zu lernen. Lassen Sie die folgende Zelle laufen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c37252d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a non-linearly separable dataset (XOR problem)\n",
    "def create_xor_data(num_samples_per_quadrant=25):\n",
    "    np.random.seed(0)  # for reproducibility\n",
    "    class_0 = np.random.randn(num_samples_per_quadrant, 2) + np.array([-2.2, -2.2])\n",
    "    class_1 = np.random.randn(num_samples_per_quadrant, 2) + np.array([2.2, 2.2])\n",
    "    class_2 = np.random.randn(num_samples_per_quadrant, 2) + np.array([-2.2, 2.2])\n",
    "    class_3 = np.random.randn(num_samples_per_quadrant, 2) + np.array([2.2, -2.2])\n",
    "    data = np.vstack((class_0, class_1, class_2, class_3))\n",
    "    labels = np.array([0]*num_samples_per_quadrant + [0]*num_samples_per_quadrant +\n",
    "                      [1]*num_samples_per_quadrant + [1]*num_samples_per_quadrant)\n",
    "    return data, labels\n",
    "X_2, y_2 = create_xor_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe5ea4e",
   "metadata": {},
   "source": [
    "### Aufgabe 6\n",
    "\n",
    "Testen Sie wieder die vier Aktivierungsfunktionen aus.\n",
    "Mit welchen ist das MLP fähig die Daten zu separieren?\n",
    "\n",
    "- identity:\n",
    "- relu\n",
    "- logistic:\n",
    "- tanh: \n",
    "\n",
    "Wie scheint sich ein MLP mit identity Aktivierungsfunktion zu verhalten?\n",
    "> Immer noch wie ein einzelnes Perzeptron da die Decision Boundary immer noch linear ist.\n",
    "\n",
    "> identity: nein, immer noch eine gerade boundary\n",
    "> relu: funktioniert\n",
    "> logistic: funktioniert\n",
    "> tanh: funktioniert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933e72ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einige Datenpunkte im XOR-Muster hinzufügen\n",
    "\n",
    "\n",
    "\n",
    "mlp_xor = MLPClassifier(hidden_layer_sizes=(10,), activation='tanh', solver='lbfgs', max_iter=200, random_state=42)\n",
    "mlp_xor.fit(X_2, y_2)\n",
    "plot_mlp_decision_boundary(mlp_xor, X_2, y_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce437362",
   "metadata": {},
   "source": [
    "## Kontrollfragen: Perzeptron und MLPs\n",
    "\n",
    "**Kontrollfrage 1**\n",
    "\n",
    "Ein Perzeptron hat drei Inputs. Wie viele lernbare Gewichte besitzt es?\n",
    "\n",
    "**Kontrollfrage 2**\n",
    "\n",
    "Beschreiben Sie das XOR-Problem in eigenen Worten und weshalb ein einzelnes Perzeptron dieses nicht lösen kann.\n",
    "\n",
    "**Kontrollfrage 3**\n",
    "\n",
    "Ein MLP besitzt einen Inputlayer mit 5 Inputs. Es hat einen Hidden-Layer mit 10 künstlichen Neuronen und dann einen Output Layer mit einem Output.\n",
    "Wie viele Gewichte gibt es im ganzen MLP?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "praktikum_ef",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
