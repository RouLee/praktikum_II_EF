{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b9880fe",
   "metadata": {},
   "source": [
    "# Komponenten eines neuronalen Netzwerkes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4453fb",
   "metadata": {},
   "source": [
    "<div style=\"padding: 5px; border: 5px solid #a10000ff;\">\n",
    "\n",
    "**Hinweis:** In den Codezellen sind jeweils einige Codeteile nicht programmiert. Diesen Code müssen Sie ergänzen. Die jeweiligen Stellen sind mit einem Kommentar und dem Keyword **TODO** vermerkt und z.T. Stellen mit ... markiert.\n",
    "\n",
    "Ausserdem gibt es einige assert Statements. Diese geben einen Fehler aus, sollte etwas bei Ihrer Programmierung nicht korrekt sein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda0f5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73722808",
   "metadata": {},
   "source": [
    "## Perzeptronen\n",
    "\n",
    "Perzeptronen sind künstliche Neuronen. Diese kommen in vielen neuronalen Netzwerken als kleinste Bausteine zum Einsatz.\n",
    "\n",
    "Ein Perzeptron bimmt eine Anzahl von numerischen Inputs entgegen, multipliziert diese je mit einem Gewicht und bildet danach eine Summe aus diesen Ergebnissen. Darauf wird die Aktivierungsfunktion des Perzeptrons angewendet um die Ausgabe des Perzeptrons zu bestimmen. In der folgenden Darstellung wird dies grafisch dargestellt. Eine Beispielberechnung von Input bis Output folgt unterhalb dieser Zelle.\n",
    "\n",
    "<img src=\"images/perzeptron_1.png\" width=\"700px\">\n",
    "\n",
    "\n",
    "\n",
    "**Gewichte (Weights)**\n",
    "\n",
    "Die Gewichte definieren, wie stark jeder Input (ein einzelnes Features) gewichtet wird und somit den Output des Perzeptrons beeinflusst. \n",
    "Das Perzeptron lernt diese Gewichte und passt diese anhand der Wichtigkeit für die Summe an.\n",
    "\n",
    "**Bias**\n",
    "\n",
    "Jedes Perzeptron hat noch einen Bias, welcher ein Gewicht darstellt, welches unabhängig vom Input ist und somit die Aktivierung unabhängig vom den Inputs steuern kann.\n",
    "\n",
    "**Aktivierungsfunktion**\n",
    "\n",
    "Die Aktivierungsfunktion entscheidet, wie stark der Output des Perzeptrons aufgrund des Ergebnisses der gewichteten Summe sein soll. Dies können kontinuierliche Outputs z.B. $[-1,1]$ sein oder auch diskrete (kategorische) -1 oder 1. \n",
    "\n",
    "Für die Aktivierungsfunktion gibt es verschiedene Varianten. In dem Beispiel oben wird die `sign()` Funktion genutzt.\n",
    "Weitere Aktivierungsfunktionen sind zum Beispiel:\n",
    "\n",
    "- $\\text{Linear:} f(z)=z $\n",
    "- $\\text{Logistic (Sigmoid): } f(z)=\\frac{1}{1+e^{-z}}$\n",
    "- $\\text{LeakyReLU: } f(z) =\n",
    "\\begin{cases}\n",
    "z, & \\text{if } z \\ge 0 \\\\\n",
    "\\alpha x, & \\text{if } z < 0\n",
    "\\end{cases}$\n",
    "\n",
    "**Optional**:\n",
    "Auf folgender Seite finden Sie Visualisierungen zu den Aktivierungsfunktionen:\n",
    "https://www.geeksforgeeks.org/machine-learning/activation-functions-neural-networks/\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a8e851",
   "metadata": {},
   "source": [
    "### Beispielberechnung\n",
    "\n",
    "Ein Perzeptron hat zwei Inputs und somit zwei Gewichten und einem Bias. Genutzt wird die `sign()` Aktivierungsfunktion.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17093e8",
   "metadata": {},
   "source": [
    "Die Inputs und Gewichte haben folgende Werte:\n",
    "\n",
    "$x_1= 5$\n",
    "$x_2= -2$\n",
    "\n",
    "$w_0 (b) = -6$\n",
    "$w_1 = 1$\n",
    "$w_2 = -4$\n",
    "\n",
    "\n",
    "Jetzt können wir die gewichtete Summe berechnen:\n",
    "\n",
    "$ z =x_1 \\cdot w_1 + x_2 \\cdot w_2 + b = 5 \\cdot 1 + (-2) \\cdot (-4) + (-6) = 7$\n",
    "\n",
    "Danach wird auf die gewichtete Summe die Aktivierungsfunktion angewendet:\n",
    "\n",
    "$ f(z) =\\begin{cases}\n",
    "1, & \\text{if } z \\ge 0 \\\\\n",
    "-1, & \\text{if } z < 0\n",
    "\\end{cases} = 1$ \n",
    "\n",
    "Das Perzeptron hat somit für die Inputs $x_1= 5$, $x_2= -2$ den Output $1$ berechnet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05588000",
   "metadata": {},
   "source": [
    "### Optional: Aufgabe 1\n",
    "Schreiben Sie eine Funktion, welche ein simples Perzeptron mit zwei Inputs, zwei Gewichten und der Sign activation Funktion abbildet. Vervollständigen Sie dazu den folgenden Code. Die \"*assert*\" Statements generieren einen Fehler, sollte die Implementation nicht korrekt sein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e09dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_function(z):\n",
    "    # TODO Implementieren Sie die sign-Aktivierungsfunktion\n",
    "    \n",
    "    return None\n",
    "\n",
    "def perceptron(w1, w2, bias, x1, x2):\n",
    "    # TODO Implementieren Sie den Output des Perzeptrons mit der gewichteten Summe und der Aktivierungsfunktion\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "assert activation_function(3) == 1, \"Testfall 1 für Aktivierungsfunktion fehlgeschlagen\"\n",
    "assert activation_function(-2) == -1, \"Testfall 2 für Aktivierungsfunktion fehlgeschlagen\"\n",
    "assert activation_function(0) == 1, \"Testfall 3 für Aktivierungsfunktion fehlgeschlagen\"\n",
    "assert perceptron(1, -4, -6, 5, -2) == 1, \"Testfall 1 für Perzeptron fehlgeschlagen\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d11ddb",
   "metadata": {},
   "source": [
    "### Optional: Lernalgorithmus Perzeptron\n",
    "\n",
    "Die Gewichte haben wir hier als gegeben genommen. In der Realität würden diese jedoch von Trainingsdaten gelernt werden. Wir schauen uns in diesem Kurs den Lernalgorithmus des Perzeptrons nicht an, da dies in heutigen neuronalen Netzerken nicht mehr anhand des alten Algorithmus geschieht. Die heutigen neuronalen Netzwerke lernen meist anhand von \"Gradient Descent\". Dabei wird am Ende der Fehler des Netzwerkes berechnet und mittels Ableitungen der Einfluss eines einzelnen Gewichts auf den Fehler bestimmt. Dadurch wird dann das Gewicht angepasst.\n",
    "Sollten Sie trotzdem daran interessiert sein, können Sie hier unter Working genauer nachlesen, wie nun die Gewichte aktualisiert werden: https://www.geeksforgeeks.org/machine-learning/what-is-perceptron-the-simplest-artificial-neural-network/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589bee1e",
   "metadata": {},
   "source": [
    "## Verschiedene Modelle\n",
    "\n",
    "Wir möchten uns nun anschauen wie sich unterschiedliche Modelle mit unterschiedlichen Aktivierungsfunktionen anhand eines Klassifikationsproblems verhalten. \n",
    "Dabei achten wir auf die Decision Boundaries, welche unteschiedliche Formen annehmen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412b9f3b",
   "metadata": {},
   "source": [
    "### Single Perzeptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b3e648",
   "metadata": {},
   "source": [
    "Wir betrachten nun einmal ein künstliches Klassifikationsproblem. Dabei sollen die Datenpunkte in zwei unterschiedliche Klassen unterteilt werden. Dazu wird ein neues Dataset generiert mit 100 Datenpunkten. Je 50 Datenpunkte pro Klasse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08734ae8",
   "metadata": {},
   "source": [
    "#### Dataset generieren und visualisieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20215511",
   "metadata": {},
   "source": [
    "Die folgende Funktion erstellt ein Dataset, welches linear separierbar ist. Das bedeutet, es kann durch eine lineare Decision Boundary (gerade Linie) fehlerlos unterteilt werden. Lassen Sie die folgende Zelle laufen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b79208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a linearly separable dataset with two features and two classes\n",
    "def create_linearly_separable_data(num_samples_per_class=50):\n",
    "    np.random.seed(0)  # for reproducibility\n",
    "    class_0 = np.random.randn(num_samples_per_class, 2) + np.array([-2, -2])\n",
    "    class_1 = np.random.randn(num_samples_per_class, 2) + np.array([2, 2])\n",
    "    data = np.vstack((class_0, class_1))\n",
    "    labels = np.array([0]*num_samples_per_class + [1]*num_samples_per_class)\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058fec79",
   "metadata": {},
   "source": [
    "Die Daten sehen wie folgt aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf1915e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = create_linearly_separable_data()\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(data[:, 0], data[:, 1], c=labels, cmap='bwr', edgecolor='k')\n",
    "plt.title('Linearly Separable Data')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54caf049",
   "metadata": {},
   "source": [
    "### Perzeptron und Lernalgorithmus\n",
    "\n",
    "In der folgenden Zelle wurde das Perzeptron mittels Vektoroperationen programmiert.\n",
    "Lassen Sie die Zelle laufen damit die Funktionen registriert werden.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002e0a04",
   "metadata": {},
   "source": [
    "#### Lernalgorithmus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0e56d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perceptron function using vectorized operations\n",
    "def perceptron_step(weight_vector, bias, input_vector):\n",
    "    weighted_sum = np.dot(weight_vector, input_vector) + bias\n",
    "    return 1 if weighted_sum >= 0 else 0\n",
    "\n",
    "# Lernalgorithmus für das Perzeptron\n",
    "# X: input data, y: labels, learning_rate: step size, epochs: number of iterations\n",
    "def train_perceptron(X, y, learning_rate=0.1, epochs=10):\n",
    "    num_features = X.shape[1]\n",
    "    weights = np.zeros(num_features)\n",
    "    bias = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(X.shape[0]):\n",
    "            prediction = perceptron_step(weights, bias, X[i])\n",
    "            error = y[i] - prediction\n",
    "            weights += learning_rate * error * X[i]\n",
    "            bias += learning_rate * error\n",
    "    return weights, bias\n",
    "\n",
    "#plot in a scatter plot the data points with different colors for the two classes\n",
    "def plot_data_with_perc(X, y, weights=None, bias=None, title='Linearly Separable Data'):\n",
    "    if weights is not None and bias is not None:\n",
    "        # create a grid to plot the decision boundary\n",
    "        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "        Z = np.array([perceptron_step(weights, bias, np.array([x, y])) for x, y in zip(xx.ravel(), yy.ravel())])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        plt.contourf(xx, yy, Z, alpha=0.3, cmap='bwr')\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr', edgecolors='k')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.show()\n",
    "\n",
    "# evaluate the perceptron on the test set\n",
    "def evaluate_perceptron(X, y, weights, bias):\n",
    "    correct_predictions = 0\n",
    "    for i in range(X.shape[0]):\n",
    "        prediction = perceptron_step(weights, bias, X[i])\n",
    "        if prediction == y[i]:\n",
    "            correct_predictions += 1\n",
    "    accuracy = correct_predictions / X.shape[0]\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845774f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1, y_1 = create_linearly_separable_data()\n",
    "\n",
    "print(\"Dimensionen der Datenpunkte X_1:\", X_1.shape)\n",
    "print(\"Dimensionen der Labels y_1:\", y_1.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a1d42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_1, y_1, test_size=0.2, random_state=42)\n",
    "\n",
    "weights, bias = train_perceptron(X_train, y_train, learning_rate=0.1, epochs=20)\n",
    "accuracy = evaluate_perceptron(X_test, y_test, weights, bias)\n",
    "print(\"Test set accuracy:\", accuracy)\n",
    "plot_data_with_perc(X_1, y_1, weights, bias)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb37ed4",
   "metadata": {},
   "source": [
    "**Bemerkung:** Die Decision Boundary wäre eigentlich linear und gerade, wird jedoch aufgrund der Methode wie Sie plotted wird fälschlicherweise mit kleinen Stufen dargestellt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a0d65d",
   "metadata": {},
   "source": [
    "### Multilayer Perzeptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bc2ff4",
   "metadata": {},
   "source": [
    "Wir haben gesehen, dass das Perzeptron eine gerade Decision Boundary lernt. In heutigen neuronalen Netzwerken kommen sehr viele Perzeptronen zum Einsatz. Die Perzeptronen haben dabei gleich wie vorhin behandelt Gewichte, einen Bias, die gewichtete Summe und eine Aktivierungsfunktion. Die Perzeptronen werden dabei in Layern (Schichten) hintereinander geschaltet. Normalerweise sind die Neuronen von zwei angrenzenden Schichten alle miteinander je verbunden. Dabei gibt es immer eine Input-Schicht, welche die Eingaben entgegennimmt, einen oder mehrere Hidden-Layers (versteckte Schichten) und dann eine Output-Schicht, welche die Ausgabe des Netzwerkes darstellt.\n",
    "Beispiel:\n",
    "\n",
    "<img src=\"images/mlp1.png\" width=\"700px\">\n",
    "\n",
    "*Quelle: ibm.com*\n",
    "\n",
    "Für ein ganz kleines Netzwerk könnte dies so aussehen:\n",
    "\n",
    "Jedes Perzeptron selbst, hat so viele Gewichte wie Verbindungen bei ihm eingehen. Zum Beispiel wird der Output des Perceptrons P1 beim Perzeptrons P3 mit einem Gewicht multipliziert und auch der Output des Perzeptrons P2 mit einem anderen Gewicht multipliziert.\n",
    "\n",
    "<img src=\"images/mlp2.png\" width=\"700px\">\n",
    "\n",
    "In der nächsten Aufgabe sollen Sie unterschiedliche Aktivierungsfunktionen testen und deren Auswirkung beobachten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b13e517",
   "metadata": {},
   "source": [
    "### Aufgabe 2\n",
    "In der folgenden Zelle ist ein MLP Klassifikationsmodell aus der Library Scikit-Learn implementiert und auch gleich die Visualisierung dargestellt.\n",
    "\n",
    "**Frage**\n",
    "Halten Sie fest, wie sich die Decision Boundary ändert, wenn Sie die Aktivierungsfunktion im parameter `activation` anpassen:\n",
    "\n",
    "- identity:\n",
    "- relu:\n",
    "- logistic:\n",
    "- tanh:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5251048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ein Multilayer Perceptron (MLP) verwenden, um die nicht-lineare Entscheidungsgrenze zu modellieren\n",
    "\n",
    "#TODO hier anpassen:\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(4,), activation='relu', max_iter=2000, random_state=41)\n",
    "mlp.fit(X_train, y_train)\n",
    "mlp_accuracy = mlp.score(X_test, y_test)\n",
    "print(\"MLP Test set accuracy:\", mlp_accuracy)\n",
    "\n",
    "# Die Decision Boundary des MLP visualisieren\n",
    "def plot_mlp_decision_boundary(mlp, X, y):\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "    Z = mlp.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='bwr')\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr', edgecolors='k')\n",
    "    plt.title('MLP Decision Boundary')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.show()\n",
    "\n",
    "plot_mlp_decision_boundary(mlp, X_1, y_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce2caa0",
   "metadata": {},
   "source": [
    "## XOR-Problem\n",
    "Wir haben gesehen, dass die Aktivierungsfunktion einen grossen Einfluss auf den Output haben kann.\n",
    "\n",
    "In den 1960er Jahren herrschte grosser Optimismus gegenüber Künstlicher Intelligenz. Ein zentrales Modell dieser Zeit war das Perzeptron, ein einfacher neuronaler Klassifikator. Dieser konnte jedoch nur linear separierbare Probleme lösen.\n",
    "\n",
    "Das XOR-Problem (Exklusives Oder) zeigte eine fundamentale Schwäche dieses Ansatzes: Die XOR-Funktion ist nicht linear separierbar. Das bedeutet, dass kein einzelnes Perzeptron eine Entscheidungsgrenze finden kann, um XOR korrekt zu klassifizieren.\n",
    "Zur Erinnerung, XOR hat folgende Wahrheisttabelle:\n",
    "\n",
    "| A | B | A XOR B |\n",
    "|---|---|---------|\n",
    "| 0 | 0 |    0    |\n",
    "| 0 | 1 |    1    |\n",
    "| 1 | 0 |    1    |\n",
    "| 1 | 1 |    0    |\n",
    "\n",
    "Wenn Sie die folgende Code-Zelle laufen lassen, sehen Sie noch eine Grafik des XOR-Problems und dass es keine lineare decision Boundary gibt, welche die Klassen fehlerlos trennt.\n",
    "\n",
    "1969 wiesen Marvin Minsky und Seymour Papert in ihrem Buch “Perceptrons” nach, dass Perzeptronen diese Fähigkeit fehlt. Da zu dieser Zeit mehrschichtige neuronale Netze und effektive Trainingsmethoden wie Backpropagation noch nicht etabliert waren, wurde daraus der (falsche) Schluss gezogen, dass neuronale Netze grundsätzlich stark limitiert seien.\n",
    "\n",
    "Diese Erkenntnis führte zu massivem Vertrauensverlust, gekürzten Forschungsgeldern und stagnierender Entwicklung im Bereich neuronaler Netze – eine Phase, die heute als AI Winter bezeichnet wird.\n",
    "\n",
    "Erst in den 1980er Jahren, mit der Wiederentdeckung von mehrschichtigen Netzen und Backpropagation, konnte das XOR-Problem erfolgreich gelöst werden, was den Grundstein für das moderne Deep Learning legte.\n",
    "\n",
    "Wir möchten nun das XOR Problem betrachten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772e6a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xor_x, xor_y = np.array([[0,0],[0,1],[1,0],[1,1]]), np.array([0,1,1,0])\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(xor_x[:,0], xor_x[:,1], c=xor_y, cmap='bwr', edgecolors='k')\n",
    "ax.set_title('XOR Data')\n",
    "ax.set_xlabel('A')\n",
    "ax.set_ylabel('B')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bf9f48",
   "metadata": {},
   "source": [
    "Wir interpretieren nun die obigen Werte für A und B als zwei Input Features und den Wert A XOR B als Label. \n",
    "Nun möchten wir untersuchen ob und mit welcher Aktivierungsfunktion, MLPs dieses Problem lösen können. Dazu generieren wir einige Datenpunkte im XOR Muster, damit das Modell genug Daten hat um zu lernen. Lassen Sie die folgende Zelle laufen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c37252d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a non-linearly separable dataset (XOR problem)\n",
    "def create_xor_data(num_samples_per_quadrant=25):\n",
    "    np.random.seed(0)  # for reproducibility\n",
    "    class_0 = np.random.randn(num_samples_per_quadrant, 2) + np.array([-2.2, -2.2])\n",
    "    class_1 = np.random.randn(num_samples_per_quadrant, 2) + np.array([2.2, 2.2])\n",
    "    class_2 = np.random.randn(num_samples_per_quadrant, 2) + np.array([-2.2, 2.2])\n",
    "    class_3 = np.random.randn(num_samples_per_quadrant, 2) + np.array([2.2, -2.2])\n",
    "    data = np.vstack((class_0, class_1, class_2, class_3))\n",
    "    labels = np.array([0]*num_samples_per_quadrant + [0]*num_samples_per_quadrant +\n",
    "                      [1]*num_samples_per_quadrant + [1]*num_samples_per_quadrant)\n",
    "    return data, labels\n",
    "X_2, y_2 = create_xor_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe5ea4e",
   "metadata": {},
   "source": [
    "### Aufgabe 3\n",
    "\n",
    "Testen Sie wieder die vier Aktivierungsfunktionen aus.\n",
    "Mit welchen ist das MLP fähig die Daten zu separieren?\n",
    "\n",
    "- identity:\n",
    "- relu\n",
    "- logistic:\n",
    "- tanh: \n",
    "\n",
    "Wie scheint sich ein MLP mit identity Aktivierungsfunktion zu verhalten?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933e72ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_xor = MLPClassifier(hidden_layer_sizes=(10,), activation='tanh', solver='lbfgs', max_iter=200, random_state=42)\n",
    "mlp_xor.fit(X_2, y_2)\n",
    "plot_mlp_decision_boundary(mlp_xor, X_2, y_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe1ed34",
   "metadata": {},
   "source": [
    "## Key-Takeaways\n",
    "- Ein einzelnes Perzeptron hat eine lineare Decision Boundary.\n",
    "- Ein MLP mit der identity Aktivierungsfunktion hat immer noch eine lineare Decision Boundary.\n",
    "- Erst ein MLP mit einer nicht-linearen Aktivierungsfunktion kann komplexere Daten richtig klassifizieren\n",
    "- Die Wahl von verschiedenen Konfigurationen eines MLPs oder Neuronalen Netzwerkes kann entscheidend sein für die Performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce437362",
   "metadata": {},
   "source": [
    "## Kontrollfragen: Perzeptron und MLPs\n",
    "\n",
    "**Kontrollfrage 1**\n",
    "\n",
    "Ein Perzeptron hat drei Inputs. Wie viele lernbare Gewichte besitzt es?\n",
    "\n",
    "**Kontrollfrage 2**\n",
    "\n",
    "Beschreiben Sie das XOR-Problem in eigenen Worten und weshalb ein einzelnes Perzeptron dieses nicht lösen kann.\n",
    "\n",
    "**Kontrollfrage 3**\n",
    "\n",
    "Ein MLP besitzt einen Inputlayer mit 5 Inputs. Es hat einen Hidden-Layer mit 10 künstlichen Neuronen und dann einen Output Layer mit einem Output.\n",
    "Wie viele Gewichte gibt es im ganzen MLP? (Gewichte werden auch Modellparameter genannt)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "praktikum_ef",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
