{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b9880fe",
   "metadata": {},
   "source": [
    "# Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6aa9d7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055839ec",
   "metadata": {},
   "source": [
    "## Softmax-Funktion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de600c4",
   "metadata": {},
   "source": [
    "Die Softmax-Funktion ist eine mathematische Funktion, die in der Regel in der letzten Schicht eines neuronalen Netzwerks verwendet wird, insbesondere bei Klassifikationsproblemen. Sie nimmt einen Vektor von Rohwerten (Logits) und transformiert ihn in einen Vektor von Wahrscheinlichkeiten, wobei die Summe aller Wahrscheinlichkeiten gleich 1 ist.\n",
    "\n",
    "Gegeben sei ein Vektor von Ausgaben des neuronalen Netzwerkes $ z = (z_1, z_2, \\ldots, z_n) $,\n",
    "Die Funktion wird wie folgt definiert:\n",
    "$$\n",
    "\\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}\n",
    "$$\n",
    "\n",
    "In Worten ausgedrückt: Für jedes Element $ z_i $ im Vektor $ z $ berechnet die Softmax-Funktion den Exponentialwert von $ z_i $ und teilt diesen durch die Summe der Exponentialwerte aller Elemente im Vektor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e518e06",
   "metadata": {},
   "source": [
    "### Torch vorbereiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da3ca290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Wir prüfen ob eine Hardwarebeschleunigung möglich ist und verwenden diese, wenn sie verfügbar ist\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "torch.manual_seed(42) # Setze den Zufallsseed für Reproduzierbarkeit\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e1dac8",
   "metadata": {},
   "source": [
    "## MNIST Dataset vorbereiten, laden und visualisieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936bcff0",
   "metadata": {},
   "source": [
    "Wir nutzen für dieses Notebook den MNIST-Datensatz, der handgeschriebene Ziffern enthält. Die Ziffern sind in 10 Klassen (0-9) unterteilt, und jede Klasse enthält Tausende von Bildern. \n",
    "Die Bilder sind in Graustufen und haben eine Auflösung von 28x28 Pixeln.\n",
    "Das Modell muss lernen, diese Ziffern korrekt zu klassifizieren.\n",
    "\n",
    "Wir werden diesen Datensatz laden, vorbereiten und einige Beispiele visualisieren. Der Datensatz kann direkt von PyTorch heruntergeladen werden, was den Prozess vereinfacht.\n",
    "\n",
    "Quelle Dataset: [MNIST Dataset](https://pytorch.org/vision/stable/datasets.html#mnist)\n",
    "\n",
    "Originale nicht mehr existierende Webseite: [MNIST Dataset](http://yann.lecun.com/exdb/mnist/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f855ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mnist Dataset laden\n",
    "mnist_train = torchvision.datasets.MNIST(root='datasets', train=True, download=True)\n",
    "mnist_test = torchvision.datasets.MNIST(root='datasets', train=False, download=True)\n",
    "\n",
    "# Damit wir die Bilder in einem CNN verwenden können, müssen wir die Bilder in Tensoren umwandeln.\n",
    "\n",
    "transform = torchvision.transforms.ToTensor()\n",
    "\n",
    "# Mit der Bibliothek Torchvision können wird das MNIST Dataset direkt herunterladen und in einem Schritt in Tensoren umwandeln. \n",
    "# Train=True lädt den Trainingsdatensatz, Train=False lädt den Testdatensatz.\n",
    "mnist_transformed_train = torchvision.datasets.MNIST(root='datasets', train=True, download=True, transform=transform)\n",
    "mnist_transformed_test = torchvision.datasets.MNIST(root='datasets', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(mnist_transformed_train, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(mnist_transformed_test, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e356c8",
   "metadata": {},
   "source": [
    "### Modell erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48608e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO erstellen Sie mit torch.nn.sequential ein Convolutional Neural Network mit folgenden Schichten. Die Schichten werden als einzelne Parameter an torch.nn.Sequential übergeben.\n",
    "# - Convolutional Layer mit einem Eingabe-Kanal, 12 Ausgabekanälen, einem Kernel von 7x7, einem Stride von 1 und einem Padding von 0\n",
    "# - Max Pooling Layer mit einem Kernel von 2x2 und einem Stride von 2\n",
    "# - RELU Aktivierungsfunktion\n",
    "# - Convolutional Layer mit 12 Eingabekanälen, 16 Ausgabekanälen, einem Kernel von 5x5, einem Stride von 1 und einem Padding von 2\n",
    "# - Max Pooling Layer mit einem Kernel von 2x2 und einem Stride von 2\n",
    "# - RELU Aktivierungsfunktion\n",
    "# - Flatten Layer, um die Daten für die Fully Connected Layer vorzubereiten\n",
    "# - Fully Connected Layer mit der Anzahl Eingabeneuronen die Sie oben berechnet haben und 10 Ausgabeneuronen.\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(in_channels=1, out_channels=12, kernel_size=7, stride=1, padding=0),\n",
    "    torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Conv2d(in_channels=12, out_channels=16, kernel_size=5, stride=1, padding=2),\n",
    "    torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(in_features=16*5*5, out_features=10),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3164ac6",
   "metadata": {},
   "source": [
    "### Netzwerk trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "304baf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modell erfolgreich geladen!\n"
     ]
    }
   ],
   "source": [
    "# Testen ob das File für das Modell existiert und das Modell geladen werden kann, ansonsten wird das Modell trainiert und gespeichert.\n",
    "if os.path.exists(\"mnist_cnn.pth\"):\n",
    "    model.load_state_dict(torch.load(\"mnist_cnn.pth\"))\n",
    "    print(\"Modell erfolgreich geladen!\")\n",
    "else:\n",
    "    # Hyperparameter definieren\n",
    "    max_num_epochs = 10\n",
    "    learning_rate = 0.001\n",
    "    momentum = 0.9\n",
    "\n",
    "    # Optimizer und die Loss-Funktion definieren\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    # Das Modell muss noch auf die Hardware verschoben werden.\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(max_num_epochs):\n",
    "        batch_train_losses = []\n",
    "        batch_test_losses = []\n",
    "        for batch in train_loader:\n",
    "            images, labels = batch\n",
    "\n",
    "            # Wir verschieben die Bilder und Labels auf die gleiche Hardware wie das Modell (CPU oder GPU)\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # TODO vervollständigen Sie hier die nötigen Schritte, um das Modell zu trainieren\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "\n",
    "            train_loss = loss(outputs, labels)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_train_losses.append(train_loss.item())\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                images, labels = batch\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                batch_test_losses.append(loss(outputs, labels).item())\n",
    "\n",
    "        training_loss = np.mean(batch_train_losses)\n",
    "        testing_loss = np.mean(batch_test_losses)\n",
    "        train_losses.append(training_loss)\n",
    "        test_losses.append(testing_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch}: Train Loss = {training_loss:.5f}, Test Loss = {testing_loss:.5f}\")\n",
    "\n",
    "    # Modell speichern\n",
    "    torch.save(model.state_dict(), 'mnist_cnn.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52674c60",
   "metadata": {},
   "source": [
    "### Ausgaben des Modells interpretieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "327adf9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3797, -9.1240, -4.8932, -5.5875, 13.7654, -1.0710,  0.6849, -0.2571,\n",
      "        -2.5168,  5.9739])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1bb8d59ed50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 10 samples aus dem Testdatensatz vorhersagen und die Bilder mit den vorhergesagten Labels anzeigen.\n",
    "with torch.no_grad():\n",
    "    for image, target in test_loader:\n",
    "        image, target = image.to(device), target.to(device)\n",
    "        outputs = model(image)\n",
    "        break\n",
    "\n",
    "# Outputs für das erste Bild im Batch\n",
    "print(outputs[0])\n",
    "\n",
    "# Bild anzeigen\n",
    "plt.imshow(image[0].cpu().squeeze(), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c3a6bd",
   "metadata": {},
   "source": [
    "### Softmax anwenden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f0146c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Klasse 0: 0.0000007\n",
      "Klasse 1: 0.0000000\n",
      "Klasse 2: 0.0000000\n",
      "Klasse 3: 0.0000000\n",
      "Klasse 4: 0.9995828\n",
      "Klasse 5: 0.0000004\n",
      "Klasse 6: 0.0000021\n",
      "Klasse 7: 0.0000008\n",
      "Klasse 8: 0.0000001\n",
      "Klasse 9: 0.0004131\n"
     ]
    }
   ],
   "source": [
    "softmax = torch.nn.Softmax(dim=0)\n",
    "probabilities = softmax(outputs[0])\n",
    "probabilities = probabilities.cpu().numpy().round(7)\n",
    "for i in range(10):\n",
    "    print(f\"Klasse {i}: {probabilities[i]:.7f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "praktikum_ef",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
